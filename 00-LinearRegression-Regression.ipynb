{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00-LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmgpkLaSsM1xMJU0rPF60S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielpy108/MachineLearningAlgorithms/blob/master/00-LinearRegression-Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTinNrY2kQ6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaTl2grmwhiQ",
        "colab_type": "text"
      },
      "source": [
        "# Regression problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz9IfMYLwpl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_diabetes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrr7vPRtkaZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "abcb49e3-68d7-474e-dbdb-1052e1b7418b"
      },
      "source": [
        "training_data = load_diabetes()\n",
        "\n",
        "X = training_data['data']\n",
        "y = training_data['target']\n",
        "features = training_data['feature_names']\n",
        "description = training_data['DESCR']\n",
        "\n",
        "print(f'Data samples: {X.shape[0]}')\n",
        "print(f'Input features (dimenssion): {X.shape[1]}')\n",
        "print(f\"Features: {features}\")\n",
        "print(description)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data samples: 442\n",
            "Input features (dimenssion): 10\n",
            "Features: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
            ".. _diabetes_dataset:\n",
            "\n",
            "Diabetes dataset\n",
            "----------------\n",
            "\n",
            "Ten baseline variables, age, sex, body mass index, average blood\n",
            "pressure, and six blood serum measurements were obtained for each of n =\n",
            "442 diabetes patients, as well as the response of interest, a\n",
            "quantitative measure of disease progression one year after baseline.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "  :Number of Instances: 442\n",
            "\n",
            "  :Number of Attributes: First 10 columns are numeric predictive values\n",
            "\n",
            "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
            "\n",
            "  :Attribute Information:\n",
            "      - Age\n",
            "      - Sex\n",
            "      - Body mass index\n",
            "      - Average blood pressure\n",
            "      - S1\n",
            "      - S2\n",
            "      - S3\n",
            "      - S4\n",
            "      - S5\n",
            "      - S6\n",
            "\n",
            "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
            "\n",
            "Source URL:\n",
            "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
            "\n",
            "For more information see:\n",
            "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
            "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziBd_fYCzAmi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "644fdaab-bcc8-4aca-a057-f1a1737a7ffa"
      },
      "source": [
        "# Turn the data into a pandas DataFrame\n",
        "df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=features+['Output'])\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "      <th>Output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019908</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>151.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068330</td>\n",
              "      <td>-0.092204</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>-0.025930</td>\n",
              "      <td>141.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.009362</td>\n",
              "      <td>206.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.046641</td>\n",
              "      <td>135.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        age       sex       bmi        bp  ...        s4        s5        s6  Output\n",
              "0  0.038076  0.050680  0.061696  0.021872  ... -0.002592  0.019908 -0.017646   151.0\n",
              "1 -0.001882 -0.044642 -0.051474 -0.026328  ... -0.039493 -0.068330 -0.092204    75.0\n",
              "2  0.085299  0.050680  0.044451 -0.005671  ... -0.002592  0.002864 -0.025930   141.0\n",
              "3 -0.089063 -0.044642 -0.011595 -0.036656  ...  0.034309  0.022692 -0.009362   206.0\n",
              "4  0.005383 -0.044642 -0.036385  0.021872  ... -0.002592 -0.031991 -0.046641   135.0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99AvSGRI4m4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to separate the dataset into training and test samples\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, train_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIqHvzGz23Zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, the data will be turned into a pytorch tensor for the linear\n",
        "# regression model\n",
        "X_training_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_training_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FFTahG58eD",
        "colab_type": "text"
      },
      "source": [
        "## Creating the linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N4QBT4V5_IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearRegression():\n",
        "    def __init__(self, nsamples, nfeatures):\n",
        "        self.W = torch.randn(nfeatures, requires_grad=True)\n",
        "        self.b = torch.randn(1, requires_grad=True)\n",
        "    \n",
        "    def pred(self, x):\n",
        "        return x@self.W + self.b\n",
        "    \n",
        "    def loss(self, y, y_pred):\n",
        "        squared_error = (y - y_pred).pow(2)\n",
        "        N = torch.numel(squared_error)\n",
        "        return 1/(2*N) * torch.sum(squared_error)\n",
        "    \n",
        "    def fit(self, X, y, lr, epochs):\n",
        "        Loss = []\n",
        "        for e in range(epochs):\n",
        "            y_pred = self.pred(X)\n",
        "            loss = self.loss(y, y_pred)\n",
        "            Loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                self.W -=  lr*self.W.grad\n",
        "                self.b -= lr*self.b.grad\n",
        "                self.W.grad.zero_()\n",
        "                self.b.grad.zero_()\n",
        "            print(f\"Epoch: {e} - loss: {loss}\")\n",
        "        return Loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiW1qJhm63LR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "\n",
        "nsamples = X_training_tensor.shape[0]\n",
        "nfeatures = X_training_tensor.shape[1]\n",
        "\n",
        "lr = LinearRegression(nsamples, nfeatures)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGtdsG01-qv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffcfc9a5-b400-4f5c-d5b8-f261860f0f15"
      },
      "source": [
        "Loss = lr.fit(X_training_tensor, y_training_tensor, lr=0.1, epochs=1000)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 - loss: 2166.2802734375\n",
            "Epoch: 1 - loss: 2165.0078125\n",
            "Epoch: 2 - loss: 2163.7373046875\n",
            "Epoch: 3 - loss: 2162.468017578125\n",
            "Epoch: 4 - loss: 2161.201171875\n",
            "Epoch: 5 - loss: 2159.935546875\n",
            "Epoch: 6 - loss: 2158.672119140625\n",
            "Epoch: 7 - loss: 2157.41015625\n",
            "Epoch: 8 - loss: 2156.15087890625\n",
            "Epoch: 9 - loss: 2154.893798828125\n",
            "Epoch: 10 - loss: 2153.6376953125\n",
            "Epoch: 11 - loss: 2152.384033203125\n",
            "Epoch: 12 - loss: 2151.132080078125\n",
            "Epoch: 13 - loss: 2149.88232421875\n",
            "Epoch: 14 - loss: 2148.634033203125\n",
            "Epoch: 15 - loss: 2147.3876953125\n",
            "Epoch: 16 - loss: 2146.14306640625\n",
            "Epoch: 17 - loss: 2144.900390625\n",
            "Epoch: 18 - loss: 2143.659912109375\n",
            "Epoch: 19 - loss: 2142.421142578125\n",
            "Epoch: 20 - loss: 2141.184326171875\n",
            "Epoch: 21 - loss: 2139.948974609375\n",
            "Epoch: 22 - loss: 2138.715576171875\n",
            "Epoch: 23 - loss: 2137.484375\n",
            "Epoch: 24 - loss: 2136.25439453125\n",
            "Epoch: 25 - loss: 2135.026611328125\n",
            "Epoch: 26 - loss: 2133.80126953125\n",
            "Epoch: 27 - loss: 2132.577392578125\n",
            "Epoch: 28 - loss: 2131.35546875\n",
            "Epoch: 29 - loss: 2130.13427734375\n",
            "Epoch: 30 - loss: 2128.916259765625\n",
            "Epoch: 31 - loss: 2127.69921875\n",
            "Epoch: 32 - loss: 2126.48388671875\n",
            "Epoch: 33 - loss: 2125.2705078125\n",
            "Epoch: 34 - loss: 2124.059814453125\n",
            "Epoch: 35 - loss: 2122.849853515625\n",
            "Epoch: 36 - loss: 2121.642578125\n",
            "Epoch: 37 - loss: 2120.436279296875\n",
            "Epoch: 38 - loss: 2119.232421875\n",
            "Epoch: 39 - loss: 2118.030517578125\n",
            "Epoch: 40 - loss: 2116.829833984375\n",
            "Epoch: 41 - loss: 2115.630859375\n",
            "Epoch: 42 - loss: 2114.43408203125\n",
            "Epoch: 43 - loss: 2113.239501953125\n",
            "Epoch: 44 - loss: 2112.04541015625\n",
            "Epoch: 45 - loss: 2110.854248046875\n",
            "Epoch: 46 - loss: 2109.664306640625\n",
            "Epoch: 47 - loss: 2108.4765625\n",
            "Epoch: 48 - loss: 2107.2900390625\n",
            "Epoch: 49 - loss: 2106.10595703125\n",
            "Epoch: 50 - loss: 2104.923095703125\n",
            "Epoch: 51 - loss: 2103.74267578125\n",
            "Epoch: 52 - loss: 2102.56298828125\n",
            "Epoch: 53 - loss: 2101.385986328125\n",
            "Epoch: 54 - loss: 2100.21044921875\n",
            "Epoch: 55 - loss: 2099.036376953125\n",
            "Epoch: 56 - loss: 2097.864501953125\n",
            "Epoch: 57 - loss: 2096.694091796875\n",
            "Epoch: 58 - loss: 2095.525390625\n",
            "Epoch: 59 - loss: 2094.358642578125\n",
            "Epoch: 60 - loss: 2093.193603515625\n",
            "Epoch: 61 - loss: 2092.02978515625\n",
            "Epoch: 62 - loss: 2090.868408203125\n",
            "Epoch: 63 - loss: 2089.708984375\n",
            "Epoch: 64 - loss: 2088.550048828125\n",
            "Epoch: 65 - loss: 2087.3935546875\n",
            "Epoch: 66 - loss: 2086.239013671875\n",
            "Epoch: 67 - loss: 2085.0859375\n",
            "Epoch: 68 - loss: 2083.93408203125\n",
            "Epoch: 69 - loss: 2082.78515625\n",
            "Epoch: 70 - loss: 2081.63720703125\n",
            "Epoch: 71 - loss: 2080.4912109375\n",
            "Epoch: 72 - loss: 2079.3466796875\n",
            "Epoch: 73 - loss: 2078.203369140625\n",
            "Epoch: 74 - loss: 2077.0625\n",
            "Epoch: 75 - loss: 2075.923095703125\n",
            "Epoch: 76 - loss: 2074.785400390625\n",
            "Epoch: 77 - loss: 2073.649169921875\n",
            "Epoch: 78 - loss: 2072.514892578125\n",
            "Epoch: 79 - loss: 2071.3828125\n",
            "Epoch: 80 - loss: 2070.251220703125\n",
            "Epoch: 81 - loss: 2069.122802734375\n",
            "Epoch: 82 - loss: 2067.994873046875\n",
            "Epoch: 83 - loss: 2066.86865234375\n",
            "Epoch: 84 - loss: 2065.74462890625\n",
            "Epoch: 85 - loss: 2064.622314453125\n",
            "Epoch: 86 - loss: 2063.5009765625\n",
            "Epoch: 87 - loss: 2062.382080078125\n",
            "Epoch: 88 - loss: 2061.26416015625\n",
            "Epoch: 89 - loss: 2060.148193359375\n",
            "Epoch: 90 - loss: 2059.03369140625\n",
            "Epoch: 91 - loss: 2057.921630859375\n",
            "Epoch: 92 - loss: 2056.809814453125\n",
            "Epoch: 93 - loss: 2055.701416015625\n",
            "Epoch: 94 - loss: 2054.593017578125\n",
            "Epoch: 95 - loss: 2053.487548828125\n",
            "Epoch: 96 - loss: 2052.3828125\n",
            "Epoch: 97 - loss: 2051.27978515625\n",
            "Epoch: 98 - loss: 2050.178955078125\n",
            "Epoch: 99 - loss: 2049.080078125\n",
            "Epoch: 100 - loss: 2047.9810791015625\n",
            "Epoch: 101 - loss: 2046.8851318359375\n",
            "Epoch: 102 - loss: 2045.7904052734375\n",
            "Epoch: 103 - loss: 2044.697265625\n",
            "Epoch: 104 - loss: 2043.6065673828125\n",
            "Epoch: 105 - loss: 2042.516357421875\n",
            "Epoch: 106 - loss: 2041.4283447265625\n",
            "Epoch: 107 - loss: 2040.3421630859375\n",
            "Epoch: 108 - loss: 2039.2572021484375\n",
            "Epoch: 109 - loss: 2038.173583984375\n",
            "Epoch: 110 - loss: 2037.0916748046875\n",
            "Epoch: 111 - loss: 2036.01123046875\n",
            "Epoch: 112 - loss: 2034.9329833984375\n",
            "Epoch: 113 - loss: 2033.8564453125\n",
            "Epoch: 114 - loss: 2032.7806396484375\n",
            "Epoch: 115 - loss: 2031.70703125\n",
            "Epoch: 116 - loss: 2030.634765625\n",
            "Epoch: 117 - loss: 2029.56396484375\n",
            "Epoch: 118 - loss: 2028.4951171875\n",
            "Epoch: 119 - loss: 2027.427978515625\n",
            "Epoch: 120 - loss: 2026.3616943359375\n",
            "Epoch: 121 - loss: 2025.2972412109375\n",
            "Epoch: 122 - loss: 2024.2347412109375\n",
            "Epoch: 123 - loss: 2023.173583984375\n",
            "Epoch: 124 - loss: 2022.1136474609375\n",
            "Epoch: 125 - loss: 2021.0557861328125\n",
            "Epoch: 126 - loss: 2019.9991455078125\n",
            "Epoch: 127 - loss: 2018.9447021484375\n",
            "Epoch: 128 - loss: 2017.8905029296875\n",
            "Epoch: 129 - loss: 2016.8389892578125\n",
            "Epoch: 130 - loss: 2015.7889404296875\n",
            "Epoch: 131 - loss: 2014.7406005859375\n",
            "Epoch: 132 - loss: 2013.69287109375\n",
            "Epoch: 133 - loss: 2012.6474609375\n",
            "Epoch: 134 - loss: 2011.603515625\n",
            "Epoch: 135 - loss: 2010.560791015625\n",
            "Epoch: 136 - loss: 2009.5198974609375\n",
            "Epoch: 137 - loss: 2008.480224609375\n",
            "Epoch: 138 - loss: 2007.4417724609375\n",
            "Epoch: 139 - loss: 2006.4052734375\n",
            "Epoch: 140 - loss: 2005.3709716796875\n",
            "Epoch: 141 - loss: 2004.33740234375\n",
            "Epoch: 142 - loss: 2003.3056640625\n",
            "Epoch: 143 - loss: 2002.275390625\n",
            "Epoch: 144 - loss: 2001.246337890625\n",
            "Epoch: 145 - loss: 2000.2191162109375\n",
            "Epoch: 146 - loss: 1999.19287109375\n",
            "Epoch: 147 - loss: 1998.1693115234375\n",
            "Epoch: 148 - loss: 1997.146240234375\n",
            "Epoch: 149 - loss: 1996.124755859375\n",
            "Epoch: 150 - loss: 1995.1048583984375\n",
            "Epoch: 151 - loss: 1994.0869140625\n",
            "Epoch: 152 - loss: 1993.0697021484375\n",
            "Epoch: 153 - loss: 1992.054443359375\n",
            "Epoch: 154 - loss: 1991.041259765625\n",
            "Epoch: 155 - loss: 1990.028076171875\n",
            "Epoch: 156 - loss: 1989.01708984375\n",
            "Epoch: 157 - loss: 1988.0078125\n",
            "Epoch: 158 - loss: 1987.000244140625\n",
            "Epoch: 159 - loss: 1985.99365234375\n",
            "Epoch: 160 - loss: 1984.98828125\n",
            "Epoch: 161 - loss: 1983.9849853515625\n",
            "Epoch: 162 - loss: 1982.9827880859375\n",
            "Epoch: 163 - loss: 1981.982421875\n",
            "Epoch: 164 - loss: 1980.9827880859375\n",
            "Epoch: 165 - loss: 1979.986083984375\n",
            "Epoch: 166 - loss: 1978.98974609375\n",
            "Epoch: 167 - loss: 1977.9952392578125\n",
            "Epoch: 168 - loss: 1977.001708984375\n",
            "Epoch: 169 - loss: 1976.0101318359375\n",
            "Epoch: 170 - loss: 1975.01953125\n",
            "Epoch: 171 - loss: 1974.0306396484375\n",
            "Epoch: 172 - loss: 1973.0435791015625\n",
            "Epoch: 173 - loss: 1972.0570068359375\n",
            "Epoch: 174 - loss: 1971.0728759765625\n",
            "Epoch: 175 - loss: 1970.0897216796875\n",
            "Epoch: 176 - loss: 1969.1077880859375\n",
            "Epoch: 177 - loss: 1968.1279296875\n",
            "Epoch: 178 - loss: 1967.148681640625\n",
            "Epoch: 179 - loss: 1966.171630859375\n",
            "Epoch: 180 - loss: 1965.1953125\n",
            "Epoch: 181 - loss: 1964.220703125\n",
            "Epoch: 182 - loss: 1963.247802734375\n",
            "Epoch: 183 - loss: 1962.2760009765625\n",
            "Epoch: 184 - loss: 1961.3056640625\n",
            "Epoch: 185 - loss: 1960.3370361328125\n",
            "Epoch: 186 - loss: 1959.369140625\n",
            "Epoch: 187 - loss: 1958.4034423828125\n",
            "Epoch: 188 - loss: 1957.4388427734375\n",
            "Epoch: 189 - loss: 1956.4757080078125\n",
            "Epoch: 190 - loss: 1955.5135498046875\n",
            "Epoch: 191 - loss: 1954.5531005859375\n",
            "Epoch: 192 - loss: 1953.5941162109375\n",
            "Epoch: 193 - loss: 1952.6370849609375\n",
            "Epoch: 194 - loss: 1951.6806640625\n",
            "Epoch: 195 - loss: 1950.7252197265625\n",
            "Epoch: 196 - loss: 1949.7720947265625\n",
            "Epoch: 197 - loss: 1948.8201904296875\n",
            "Epoch: 198 - loss: 1947.8697509765625\n",
            "Epoch: 199 - loss: 1946.919677734375\n",
            "Epoch: 200 - loss: 1945.9718017578125\n",
            "Epoch: 201 - loss: 1945.0252685546875\n",
            "Epoch: 202 - loss: 1944.080078125\n",
            "Epoch: 203 - loss: 1943.1357421875\n",
            "Epoch: 204 - loss: 1942.1943359375\n",
            "Epoch: 205 - loss: 1941.2530517578125\n",
            "Epoch: 206 - loss: 1940.3128662109375\n",
            "Epoch: 207 - loss: 1939.3753662109375\n",
            "Epoch: 208 - loss: 1938.4384765625\n",
            "Epoch: 209 - loss: 1937.5023193359375\n",
            "Epoch: 210 - loss: 1936.5682373046875\n",
            "Epoch: 211 - loss: 1935.635498046875\n",
            "Epoch: 212 - loss: 1934.7037353515625\n",
            "Epoch: 213 - loss: 1933.7733154296875\n",
            "Epoch: 214 - loss: 1932.8450927734375\n",
            "Epoch: 215 - loss: 1931.9173583984375\n",
            "Epoch: 216 - loss: 1930.9912109375\n",
            "Epoch: 217 - loss: 1930.06640625\n",
            "Epoch: 218 - loss: 1929.1431884765625\n",
            "Epoch: 219 - loss: 1928.2210693359375\n",
            "Epoch: 220 - loss: 1927.3001708984375\n",
            "Epoch: 221 - loss: 1926.38037109375\n",
            "Epoch: 222 - loss: 1925.462646484375\n",
            "Epoch: 223 - loss: 1924.545654296875\n",
            "Epoch: 224 - loss: 1923.6307373046875\n",
            "Epoch: 225 - loss: 1922.716552734375\n",
            "Epoch: 226 - loss: 1921.803466796875\n",
            "Epoch: 227 - loss: 1920.8924560546875\n",
            "Epoch: 228 - loss: 1919.9822998046875\n",
            "Epoch: 229 - loss: 1919.0732421875\n",
            "Epoch: 230 - loss: 1918.1654052734375\n",
            "Epoch: 231 - loss: 1917.259521484375\n",
            "Epoch: 232 - loss: 1916.3546142578125\n",
            "Epoch: 233 - loss: 1915.450927734375\n",
            "Epoch: 234 - loss: 1914.548828125\n",
            "Epoch: 235 - loss: 1913.6478271484375\n",
            "Epoch: 236 - loss: 1912.7479248046875\n",
            "Epoch: 237 - loss: 1911.8494873046875\n",
            "Epoch: 238 - loss: 1910.9525146484375\n",
            "Epoch: 239 - loss: 1910.056640625\n",
            "Epoch: 240 - loss: 1909.1619873046875\n",
            "Epoch: 241 - loss: 1908.2689208984375\n",
            "Epoch: 242 - loss: 1907.3773193359375\n",
            "Epoch: 243 - loss: 1906.486328125\n",
            "Epoch: 244 - loss: 1905.59716796875\n",
            "Epoch: 245 - loss: 1904.708740234375\n",
            "Epoch: 246 - loss: 1903.8221435546875\n",
            "Epoch: 247 - loss: 1902.937255859375\n",
            "Epoch: 248 - loss: 1902.052490234375\n",
            "Epoch: 249 - loss: 1901.16943359375\n",
            "Epoch: 250 - loss: 1900.2882080078125\n",
            "Epoch: 251 - loss: 1899.4075927734375\n",
            "Epoch: 252 - loss: 1898.528076171875\n",
            "Epoch: 253 - loss: 1897.650634765625\n",
            "Epoch: 254 - loss: 1896.7740478515625\n",
            "Epoch: 255 - loss: 1895.8984375\n",
            "Epoch: 256 - loss: 1895.0245361328125\n",
            "Epoch: 257 - loss: 1894.15185546875\n",
            "Epoch: 258 - loss: 1893.280029296875\n",
            "Epoch: 259 - loss: 1892.410400390625\n",
            "Epoch: 260 - loss: 1891.5408935546875\n",
            "Epoch: 261 - loss: 1890.6729736328125\n",
            "Epoch: 262 - loss: 1889.806640625\n",
            "Epoch: 263 - loss: 1888.9412841796875\n",
            "Epoch: 264 - loss: 1888.0771484375\n",
            "Epoch: 265 - loss: 1887.2142333984375\n",
            "Epoch: 266 - loss: 1886.3529052734375\n",
            "Epoch: 267 - loss: 1885.4925537109375\n",
            "Epoch: 268 - loss: 1884.6334228515625\n",
            "Epoch: 269 - loss: 1883.7752685546875\n",
            "Epoch: 270 - loss: 1882.9185791015625\n",
            "Epoch: 271 - loss: 1882.063232421875\n",
            "Epoch: 272 - loss: 1881.2088623046875\n",
            "Epoch: 273 - loss: 1880.356201171875\n",
            "Epoch: 274 - loss: 1879.5045166015625\n",
            "Epoch: 275 - loss: 1878.6541748046875\n",
            "Epoch: 276 - loss: 1877.804443359375\n",
            "Epoch: 277 - loss: 1876.95654296875\n",
            "Epoch: 278 - loss: 1876.1092529296875\n",
            "Epoch: 279 - loss: 1875.263427734375\n",
            "Epoch: 280 - loss: 1874.41943359375\n",
            "Epoch: 281 - loss: 1873.576171875\n",
            "Epoch: 282 - loss: 1872.734130859375\n",
            "Epoch: 283 - loss: 1871.8935546875\n",
            "Epoch: 284 - loss: 1871.0538330078125\n",
            "Epoch: 285 - loss: 1870.21484375\n",
            "Epoch: 286 - loss: 1869.3782958984375\n",
            "Epoch: 287 - loss: 1868.541748046875\n",
            "Epoch: 288 - loss: 1867.70703125\n",
            "Epoch: 289 - loss: 1866.8736572265625\n",
            "Epoch: 290 - loss: 1866.0413818359375\n",
            "Epoch: 291 - loss: 1865.2103271484375\n",
            "Epoch: 292 - loss: 1864.3802490234375\n",
            "Epoch: 293 - loss: 1863.551025390625\n",
            "Epoch: 294 - loss: 1862.7236328125\n",
            "Epoch: 295 - loss: 1861.896728515625\n",
            "Epoch: 296 - loss: 1861.072021484375\n",
            "Epoch: 297 - loss: 1860.247802734375\n",
            "Epoch: 298 - loss: 1859.424560546875\n",
            "Epoch: 299 - loss: 1858.6029052734375\n",
            "Epoch: 300 - loss: 1857.7822265625\n",
            "Epoch: 301 - loss: 1856.963134765625\n",
            "Epoch: 302 - loss: 1856.144775390625\n",
            "Epoch: 303 - loss: 1855.3275146484375\n",
            "Epoch: 304 - loss: 1854.511474609375\n",
            "Epoch: 305 - loss: 1853.6966552734375\n",
            "Epoch: 306 - loss: 1852.8831787109375\n",
            "Epoch: 307 - loss: 1852.0706787109375\n",
            "Epoch: 308 - loss: 1851.2596435546875\n",
            "Epoch: 309 - loss: 1850.448974609375\n",
            "Epoch: 310 - loss: 1849.6405029296875\n",
            "Epoch: 311 - loss: 1848.83203125\n",
            "Epoch: 312 - loss: 1848.0260009765625\n",
            "Epoch: 313 - loss: 1847.22021484375\n",
            "Epoch: 314 - loss: 1846.4154052734375\n",
            "Epoch: 315 - loss: 1845.61279296875\n",
            "Epoch: 316 - loss: 1844.8104248046875\n",
            "Epoch: 317 - loss: 1844.0096435546875\n",
            "Epoch: 318 - loss: 1843.209716796875\n",
            "Epoch: 319 - loss: 1842.4111328125\n",
            "Epoch: 320 - loss: 1841.613525390625\n",
            "Epoch: 321 - loss: 1840.8175048828125\n",
            "Epoch: 322 - loss: 1840.0220947265625\n",
            "Epoch: 323 - loss: 1839.227294921875\n",
            "Epoch: 324 - loss: 1838.4349365234375\n",
            "Epoch: 325 - loss: 1837.642578125\n",
            "Epoch: 326 - loss: 1836.8519287109375\n",
            "Epoch: 327 - loss: 1836.0623779296875\n",
            "Epoch: 328 - loss: 1835.273681640625\n",
            "Epoch: 329 - loss: 1834.486083984375\n",
            "Epoch: 330 - loss: 1833.7003173828125\n",
            "Epoch: 331 - loss: 1832.914794921875\n",
            "Epoch: 332 - loss: 1832.131103515625\n",
            "Epoch: 333 - loss: 1831.3480224609375\n",
            "Epoch: 334 - loss: 1830.56640625\n",
            "Epoch: 335 - loss: 1829.784912109375\n",
            "Epoch: 336 - loss: 1829.006103515625\n",
            "Epoch: 337 - loss: 1828.227294921875\n",
            "Epoch: 338 - loss: 1827.4501953125\n",
            "Epoch: 339 - loss: 1826.6732177734375\n",
            "Epoch: 340 - loss: 1825.898193359375\n",
            "Epoch: 341 - loss: 1825.1243896484375\n",
            "Epoch: 342 - loss: 1824.35107421875\n",
            "Epoch: 343 - loss: 1823.5794677734375\n",
            "Epoch: 344 - loss: 1822.80859375\n",
            "Epoch: 345 - loss: 1822.039306640625\n",
            "Epoch: 346 - loss: 1821.2698974609375\n",
            "Epoch: 347 - loss: 1820.5025634765625\n",
            "Epoch: 348 - loss: 1819.7362060546875\n",
            "Epoch: 349 - loss: 1818.970947265625\n",
            "Epoch: 350 - loss: 1818.20654296875\n",
            "Epoch: 351 - loss: 1817.44287109375\n",
            "Epoch: 352 - loss: 1816.6810302734375\n",
            "Epoch: 353 - loss: 1815.919921875\n",
            "Epoch: 354 - loss: 1815.15966796875\n",
            "Epoch: 355 - loss: 1814.4010009765625\n",
            "Epoch: 356 - loss: 1813.6435546875\n",
            "Epoch: 357 - loss: 1812.88671875\n",
            "Epoch: 358 - loss: 1812.1307373046875\n",
            "Epoch: 359 - loss: 1811.3763427734375\n",
            "Epoch: 360 - loss: 1810.6224365234375\n",
            "Epoch: 361 - loss: 1809.869873046875\n",
            "Epoch: 362 - loss: 1809.1182861328125\n",
            "Epoch: 363 - loss: 1808.367919921875\n",
            "Epoch: 364 - loss: 1807.61865234375\n",
            "Epoch: 365 - loss: 1806.8701171875\n",
            "Epoch: 366 - loss: 1806.123291015625\n",
            "Epoch: 367 - loss: 1805.3768310546875\n",
            "Epoch: 368 - loss: 1804.6322021484375\n",
            "Epoch: 369 - loss: 1803.8876953125\n",
            "Epoch: 370 - loss: 1803.1444091796875\n",
            "Epoch: 371 - loss: 1802.4033203125\n",
            "Epoch: 372 - loss: 1801.662109375\n",
            "Epoch: 373 - loss: 1800.9217529296875\n",
            "Epoch: 374 - loss: 1800.1829833984375\n",
            "Epoch: 375 - loss: 1799.4453125\n",
            "Epoch: 376 - loss: 1798.7081298828125\n",
            "Epoch: 377 - loss: 1797.97265625\n",
            "Epoch: 378 - loss: 1797.23779296875\n",
            "Epoch: 379 - loss: 1796.5042724609375\n",
            "Epoch: 380 - loss: 1795.771728515625\n",
            "Epoch: 381 - loss: 1795.0401611328125\n",
            "Epoch: 382 - loss: 1794.309326171875\n",
            "Epoch: 383 - loss: 1793.579833984375\n",
            "Epoch: 384 - loss: 1792.851318359375\n",
            "Epoch: 385 - loss: 1792.1236572265625\n",
            "Epoch: 386 - loss: 1791.3978271484375\n",
            "Epoch: 387 - loss: 1790.671875\n",
            "Epoch: 388 - loss: 1789.947998046875\n",
            "Epoch: 389 - loss: 1789.22412109375\n",
            "Epoch: 390 - loss: 1788.501708984375\n",
            "Epoch: 391 - loss: 1787.780029296875\n",
            "Epoch: 392 - loss: 1787.0596923828125\n",
            "Epoch: 393 - loss: 1786.3408203125\n",
            "Epoch: 394 - loss: 1785.621826171875\n",
            "Epoch: 395 - loss: 1784.9049072265625\n",
            "Epoch: 396 - loss: 1784.1881103515625\n",
            "Epoch: 397 - loss: 1783.47265625\n",
            "Epoch: 398 - loss: 1782.758544921875\n",
            "Epoch: 399 - loss: 1782.045166015625\n",
            "Epoch: 400 - loss: 1781.3330078125\n",
            "Epoch: 401 - loss: 1780.6217041015625\n",
            "Epoch: 402 - loss: 1779.9112548828125\n",
            "Epoch: 403 - loss: 1779.201904296875\n",
            "Epoch: 404 - loss: 1778.4932861328125\n",
            "Epoch: 405 - loss: 1777.785888671875\n",
            "Epoch: 406 - loss: 1777.0794677734375\n",
            "Epoch: 407 - loss: 1776.3740234375\n",
            "Epoch: 408 - loss: 1775.669921875\n",
            "Epoch: 409 - loss: 1774.96630859375\n",
            "Epoch: 410 - loss: 1774.2635498046875\n",
            "Epoch: 411 - loss: 1773.5623779296875\n",
            "Epoch: 412 - loss: 1772.862060546875\n",
            "Epoch: 413 - loss: 1772.1619873046875\n",
            "Epoch: 414 - loss: 1771.463623046875\n",
            "Epoch: 415 - loss: 1770.7659912109375\n",
            "Epoch: 416 - loss: 1770.0693359375\n",
            "Epoch: 417 - loss: 1769.373779296875\n",
            "Epoch: 418 - loss: 1768.67919921875\n",
            "Epoch: 419 - loss: 1767.9847412109375\n",
            "Epoch: 420 - loss: 1767.29248046875\n",
            "Epoch: 421 - loss: 1766.6009521484375\n",
            "Epoch: 422 - loss: 1765.91015625\n",
            "Epoch: 423 - loss: 1765.22021484375\n",
            "Epoch: 424 - loss: 1764.531494140625\n",
            "Epoch: 425 - loss: 1763.843505859375\n",
            "Epoch: 426 - loss: 1763.156494140625\n",
            "Epoch: 427 - loss: 1762.4705810546875\n",
            "Epoch: 428 - loss: 1761.7850341796875\n",
            "Epoch: 429 - loss: 1761.1014404296875\n",
            "Epoch: 430 - loss: 1760.418212890625\n",
            "Epoch: 431 - loss: 1759.736083984375\n",
            "Epoch: 432 - loss: 1759.054443359375\n",
            "Epoch: 433 - loss: 1758.3741455078125\n",
            "Epoch: 434 - loss: 1757.6947021484375\n",
            "Epoch: 435 - loss: 1757.0167236328125\n",
            "Epoch: 436 - loss: 1756.3392333984375\n",
            "Epoch: 437 - loss: 1755.6627197265625\n",
            "Epoch: 438 - loss: 1754.9869384765625\n",
            "Epoch: 439 - loss: 1754.3123779296875\n",
            "Epoch: 440 - loss: 1753.638916015625\n",
            "Epoch: 441 - loss: 1752.9659423828125\n",
            "Epoch: 442 - loss: 1752.29443359375\n",
            "Epoch: 443 - loss: 1751.62255859375\n",
            "Epoch: 444 - loss: 1750.95263671875\n",
            "Epoch: 445 - loss: 1750.2835693359375\n",
            "Epoch: 446 - loss: 1749.6156005859375\n",
            "Epoch: 447 - loss: 1748.94873046875\n",
            "Epoch: 448 - loss: 1748.2823486328125\n",
            "Epoch: 449 - loss: 1747.61669921875\n",
            "Epoch: 450 - loss: 1746.95263671875\n",
            "Epoch: 451 - loss: 1746.28857421875\n",
            "Epoch: 452 - loss: 1745.6263427734375\n",
            "Epoch: 453 - loss: 1744.96435546875\n",
            "Epoch: 454 - loss: 1744.3037109375\n",
            "Epoch: 455 - loss: 1743.644287109375\n",
            "Epoch: 456 - loss: 1742.9849853515625\n",
            "Epoch: 457 - loss: 1742.3265380859375\n",
            "Epoch: 458 - loss: 1741.669921875\n",
            "Epoch: 459 - loss: 1741.0135498046875\n",
            "Epoch: 460 - loss: 1740.3583984375\n",
            "Epoch: 461 - loss: 1739.703857421875\n",
            "Epoch: 462 - loss: 1739.050537109375\n",
            "Epoch: 463 - loss: 1738.3975830078125\n",
            "Epoch: 464 - loss: 1737.74609375\n",
            "Epoch: 465 - loss: 1737.095458984375\n",
            "Epoch: 466 - loss: 1736.4451904296875\n",
            "Epoch: 467 - loss: 1735.79638671875\n",
            "Epoch: 468 - loss: 1735.1483154296875\n",
            "Epoch: 469 - loss: 1734.5013427734375\n",
            "Epoch: 470 - loss: 1733.8546142578125\n",
            "Epoch: 471 - loss: 1733.209228515625\n",
            "Epoch: 472 - loss: 1732.5643310546875\n",
            "Epoch: 473 - loss: 1731.9212646484375\n",
            "Epoch: 474 - loss: 1731.2783203125\n",
            "Epoch: 475 - loss: 1730.63623046875\n",
            "Epoch: 476 - loss: 1729.9954833984375\n",
            "Epoch: 477 - loss: 1729.3553466796875\n",
            "Epoch: 478 - loss: 1728.7159423828125\n",
            "Epoch: 479 - loss: 1728.07763671875\n",
            "Epoch: 480 - loss: 1727.440185546875\n",
            "Epoch: 481 - loss: 1726.8038330078125\n",
            "Epoch: 482 - loss: 1726.16796875\n",
            "Epoch: 483 - loss: 1725.5330810546875\n",
            "Epoch: 484 - loss: 1724.899169921875\n",
            "Epoch: 485 - loss: 1724.2662353515625\n",
            "Epoch: 486 - loss: 1723.6339111328125\n",
            "Epoch: 487 - loss: 1723.0023193359375\n",
            "Epoch: 488 - loss: 1722.3720703125\n",
            "Epoch: 489 - loss: 1721.741943359375\n",
            "Epoch: 490 - loss: 1721.1136474609375\n",
            "Epoch: 491 - loss: 1720.48583984375\n",
            "Epoch: 492 - loss: 1719.8587646484375\n",
            "Epoch: 493 - loss: 1719.232177734375\n",
            "Epoch: 494 - loss: 1718.6068115234375\n",
            "Epoch: 495 - loss: 1717.9830322265625\n",
            "Epoch: 496 - loss: 1717.359130859375\n",
            "Epoch: 497 - loss: 1716.736083984375\n",
            "Epoch: 498 - loss: 1716.114013671875\n",
            "Epoch: 499 - loss: 1715.4931640625\n",
            "Epoch: 500 - loss: 1714.873046875\n",
            "Epoch: 501 - loss: 1714.253662109375\n",
            "Epoch: 502 - loss: 1713.635009765625\n",
            "Epoch: 503 - loss: 1713.017822265625\n",
            "Epoch: 504 - loss: 1712.400390625\n",
            "Epoch: 505 - loss: 1711.7845458984375\n",
            "Epoch: 506 - loss: 1711.169921875\n",
            "Epoch: 507 - loss: 1710.5552978515625\n",
            "Epoch: 508 - loss: 1709.9417724609375\n",
            "Epoch: 509 - loss: 1709.3292236328125\n",
            "Epoch: 510 - loss: 1708.717529296875\n",
            "Epoch: 511 - loss: 1708.1065673828125\n",
            "Epoch: 512 - loss: 1707.4964599609375\n",
            "Epoch: 513 - loss: 1706.8873291015625\n",
            "Epoch: 514 - loss: 1706.279052734375\n",
            "Epoch: 515 - loss: 1705.6712646484375\n",
            "Epoch: 516 - loss: 1705.0643310546875\n",
            "Epoch: 517 - loss: 1704.45849609375\n",
            "Epoch: 518 - loss: 1703.853271484375\n",
            "Epoch: 519 - loss: 1703.2490234375\n",
            "Epoch: 520 - loss: 1702.6458740234375\n",
            "Epoch: 521 - loss: 1702.042724609375\n",
            "Epoch: 522 - loss: 1701.44091796875\n",
            "Epoch: 523 - loss: 1700.8404541015625\n",
            "Epoch: 524 - loss: 1700.23974609375\n",
            "Epoch: 525 - loss: 1699.6405029296875\n",
            "Epoch: 526 - loss: 1699.0419921875\n",
            "Epoch: 527 - loss: 1698.444091796875\n",
            "Epoch: 528 - loss: 1697.84765625\n",
            "Epoch: 529 - loss: 1697.2509765625\n",
            "Epoch: 530 - loss: 1696.656005859375\n",
            "Epoch: 531 - loss: 1696.061279296875\n",
            "Epoch: 532 - loss: 1695.467529296875\n",
            "Epoch: 533 - loss: 1694.874755859375\n",
            "Epoch: 534 - loss: 1694.2830810546875\n",
            "Epoch: 535 - loss: 1693.6917724609375\n",
            "Epoch: 536 - loss: 1693.1009521484375\n",
            "Epoch: 537 - loss: 1692.511474609375\n",
            "Epoch: 538 - loss: 1691.9224853515625\n",
            "Epoch: 539 - loss: 1691.334716796875\n",
            "Epoch: 540 - loss: 1690.746826171875\n",
            "Epoch: 541 - loss: 1690.1607666015625\n",
            "Epoch: 542 - loss: 1689.5751953125\n",
            "Epoch: 543 - loss: 1688.990234375\n",
            "Epoch: 544 - loss: 1688.4061279296875\n",
            "Epoch: 545 - loss: 1687.8228759765625\n",
            "Epoch: 546 - loss: 1687.240478515625\n",
            "Epoch: 547 - loss: 1686.658935546875\n",
            "Epoch: 548 - loss: 1686.0780029296875\n",
            "Epoch: 549 - loss: 1685.497802734375\n",
            "Epoch: 550 - loss: 1684.91796875\n",
            "Epoch: 551 - loss: 1684.3397216796875\n",
            "Epoch: 552 - loss: 1683.761962890625\n",
            "Epoch: 553 - loss: 1683.1846923828125\n",
            "Epoch: 554 - loss: 1682.609130859375\n",
            "Epoch: 555 - loss: 1682.0330810546875\n",
            "Epoch: 556 - loss: 1681.45849609375\n",
            "Epoch: 557 - loss: 1680.884765625\n",
            "Epoch: 558 - loss: 1680.3116455078125\n",
            "Epoch: 559 - loss: 1679.7392578125\n",
            "Epoch: 560 - loss: 1679.167724609375\n",
            "Epoch: 561 - loss: 1678.5968017578125\n",
            "Epoch: 562 - loss: 1678.027099609375\n",
            "Epoch: 563 - loss: 1677.457763671875\n",
            "Epoch: 564 - loss: 1676.8892822265625\n",
            "Epoch: 565 - loss: 1676.32177734375\n",
            "Epoch: 566 - loss: 1675.754638671875\n",
            "Epoch: 567 - loss: 1675.1885986328125\n",
            "Epoch: 568 - loss: 1674.6234130859375\n",
            "Epoch: 569 - loss: 1674.0582275390625\n",
            "Epoch: 570 - loss: 1673.4947509765625\n",
            "Epoch: 571 - loss: 1672.9315185546875\n",
            "Epoch: 572 - loss: 1672.3687744140625\n",
            "Epoch: 573 - loss: 1671.8076171875\n",
            "Epoch: 574 - loss: 1671.2467041015625\n",
            "Epoch: 575 - loss: 1670.686279296875\n",
            "Epoch: 576 - loss: 1670.127197265625\n",
            "Epoch: 577 - loss: 1669.568359375\n",
            "Epoch: 578 - loss: 1669.010498046875\n",
            "Epoch: 579 - loss: 1668.4534912109375\n",
            "Epoch: 580 - loss: 1667.8974609375\n",
            "Epoch: 581 - loss: 1667.342041015625\n",
            "Epoch: 582 - loss: 1666.7867431640625\n",
            "Epoch: 583 - loss: 1666.2327880859375\n",
            "Epoch: 584 - loss: 1665.6795654296875\n",
            "Epoch: 585 - loss: 1665.1265869140625\n",
            "Epoch: 586 - loss: 1664.5745849609375\n",
            "Epoch: 587 - loss: 1664.0240478515625\n",
            "Epoch: 588 - loss: 1663.4739990234375\n",
            "Epoch: 589 - loss: 1662.924072265625\n",
            "Epoch: 590 - loss: 1662.3748779296875\n",
            "Epoch: 591 - loss: 1661.8265380859375\n",
            "Epoch: 592 - loss: 1661.2789306640625\n",
            "Epoch: 593 - loss: 1660.732421875\n",
            "Epoch: 594 - loss: 1660.1866455078125\n",
            "Epoch: 595 - loss: 1659.6409912109375\n",
            "Epoch: 596 - loss: 1659.0968017578125\n",
            "Epoch: 597 - loss: 1658.5533447265625\n",
            "Epoch: 598 - loss: 1658.0103759765625\n",
            "Epoch: 599 - loss: 1657.4677734375\n",
            "Epoch: 600 - loss: 1656.926025390625\n",
            "Epoch: 601 - loss: 1656.3853759765625\n",
            "Epoch: 602 - loss: 1655.84521484375\n",
            "Epoch: 603 - loss: 1655.3056640625\n",
            "Epoch: 604 - loss: 1654.7667236328125\n",
            "Epoch: 605 - loss: 1654.229248046875\n",
            "Epoch: 606 - loss: 1653.691650390625\n",
            "Epoch: 607 - loss: 1653.1552734375\n",
            "Epoch: 608 - loss: 1652.619384765625\n",
            "Epoch: 609 - loss: 1652.084228515625\n",
            "Epoch: 610 - loss: 1651.5499267578125\n",
            "Epoch: 611 - loss: 1651.0162353515625\n",
            "Epoch: 612 - loss: 1650.4835205078125\n",
            "Epoch: 613 - loss: 1649.9508056640625\n",
            "Epoch: 614 - loss: 1649.4190673828125\n",
            "Epoch: 615 - loss: 1648.888671875\n",
            "Epoch: 616 - loss: 1648.3583984375\n",
            "Epoch: 617 - loss: 1647.8291015625\n",
            "Epoch: 618 - loss: 1647.30029296875\n",
            "Epoch: 619 - loss: 1646.7725830078125\n",
            "Epoch: 620 - loss: 1646.244873046875\n",
            "Epoch: 621 - loss: 1645.7183837890625\n",
            "Epoch: 622 - loss: 1645.1927490234375\n",
            "Epoch: 623 - loss: 1644.6673583984375\n",
            "Epoch: 624 - loss: 1644.1429443359375\n",
            "Epoch: 625 - loss: 1643.6187744140625\n",
            "Epoch: 626 - loss: 1643.0963134765625\n",
            "Epoch: 627 - loss: 1642.5733642578125\n",
            "Epoch: 628 - loss: 1642.051513671875\n",
            "Epoch: 629 - loss: 1641.53076171875\n",
            "Epoch: 630 - loss: 1641.010498046875\n",
            "Epoch: 631 - loss: 1640.4908447265625\n",
            "Epoch: 632 - loss: 1639.97216796875\n",
            "Epoch: 633 - loss: 1639.4537353515625\n",
            "Epoch: 634 - loss: 1638.9359130859375\n",
            "Epoch: 635 - loss: 1638.4190673828125\n",
            "Epoch: 636 - loss: 1637.9027099609375\n",
            "Epoch: 637 - loss: 1637.387451171875\n",
            "Epoch: 638 - loss: 1636.8724365234375\n",
            "Epoch: 639 - loss: 1636.3585205078125\n",
            "Epoch: 640 - loss: 1635.8450927734375\n",
            "Epoch: 641 - loss: 1635.33203125\n",
            "Epoch: 642 - loss: 1634.81982421875\n",
            "Epoch: 643 - loss: 1634.3089599609375\n",
            "Epoch: 644 - loss: 1633.7979736328125\n",
            "Epoch: 645 - loss: 1633.287841796875\n",
            "Epoch: 646 - loss: 1632.7786865234375\n",
            "Epoch: 647 - loss: 1632.269775390625\n",
            "Epoch: 648 - loss: 1631.7615966796875\n",
            "Epoch: 649 - loss: 1631.2545166015625\n",
            "Epoch: 650 - loss: 1630.7479248046875\n",
            "Epoch: 651 - loss: 1630.24169921875\n",
            "Epoch: 652 - loss: 1629.7359619140625\n",
            "Epoch: 653 - loss: 1629.2315673828125\n",
            "Epoch: 654 - loss: 1628.7276611328125\n",
            "Epoch: 655 - loss: 1628.22412109375\n",
            "Epoch: 656 - loss: 1627.721435546875\n",
            "Epoch: 657 - loss: 1627.219482421875\n",
            "Epoch: 658 - loss: 1626.7181396484375\n",
            "Epoch: 659 - loss: 1626.217529296875\n",
            "Epoch: 660 - loss: 1625.717529296875\n",
            "Epoch: 661 - loss: 1625.21826171875\n",
            "Epoch: 662 - loss: 1624.719482421875\n",
            "Epoch: 663 - loss: 1624.2213134765625\n",
            "Epoch: 664 - loss: 1623.7237548828125\n",
            "Epoch: 665 - loss: 1623.2271728515625\n",
            "Epoch: 666 - loss: 1622.730712890625\n",
            "Epoch: 667 - loss: 1622.2353515625\n",
            "Epoch: 668 - loss: 1621.7406005859375\n",
            "Epoch: 669 - loss: 1621.246826171875\n",
            "Epoch: 670 - loss: 1620.7529296875\n",
            "Epoch: 671 - loss: 1620.2603759765625\n",
            "Epoch: 672 - loss: 1619.767578125\n",
            "Epoch: 673 - loss: 1619.2764892578125\n",
            "Epoch: 674 - loss: 1618.7855224609375\n",
            "Epoch: 675 - loss: 1618.295166015625\n",
            "Epoch: 676 - loss: 1617.8057861328125\n",
            "Epoch: 677 - loss: 1617.316650390625\n",
            "Epoch: 678 - loss: 1616.828369140625\n",
            "Epoch: 679 - loss: 1616.3409423828125\n",
            "Epoch: 680 - loss: 1615.8538818359375\n",
            "Epoch: 681 - loss: 1615.3671875\n",
            "Epoch: 682 - loss: 1614.881591796875\n",
            "Epoch: 683 - loss: 1614.396484375\n",
            "Epoch: 684 - loss: 1613.912353515625\n",
            "Epoch: 685 - loss: 1613.4281005859375\n",
            "Epoch: 686 - loss: 1612.94482421875\n",
            "Epoch: 687 - loss: 1612.46240234375\n",
            "Epoch: 688 - loss: 1611.98046875\n",
            "Epoch: 689 - loss: 1611.4993896484375\n",
            "Epoch: 690 - loss: 1611.0185546875\n",
            "Epoch: 691 - loss: 1610.5384521484375\n",
            "Epoch: 692 - loss: 1610.0589599609375\n",
            "Epoch: 693 - loss: 1609.580322265625\n",
            "Epoch: 694 - loss: 1609.1021728515625\n",
            "Epoch: 695 - loss: 1608.6243896484375\n",
            "Epoch: 696 - loss: 1608.1475830078125\n",
            "Epoch: 697 - loss: 1607.6712646484375\n",
            "Epoch: 698 - loss: 1607.195556640625\n",
            "Epoch: 699 - loss: 1606.7203369140625\n",
            "Epoch: 700 - loss: 1606.2459716796875\n",
            "Epoch: 701 - loss: 1605.772216796875\n",
            "Epoch: 702 - loss: 1605.2991943359375\n",
            "Epoch: 703 - loss: 1604.8265380859375\n",
            "Epoch: 704 - loss: 1604.3544921875\n",
            "Epoch: 705 - loss: 1603.8834228515625\n",
            "Epoch: 706 - loss: 1603.4127197265625\n",
            "Epoch: 707 - loss: 1602.9423828125\n",
            "Epoch: 708 - loss: 1602.472900390625\n",
            "Epoch: 709 - loss: 1602.0037841796875\n",
            "Epoch: 710 - loss: 1601.5357666015625\n",
            "Epoch: 711 - loss: 1601.068359375\n",
            "Epoch: 712 - loss: 1600.6009521484375\n",
            "Epoch: 713 - loss: 1600.1343994140625\n",
            "Epoch: 714 - loss: 1599.6690673828125\n",
            "Epoch: 715 - loss: 1599.2037353515625\n",
            "Epoch: 716 - loss: 1598.7388916015625\n",
            "Epoch: 717 - loss: 1598.2750244140625\n",
            "Epoch: 718 - loss: 1597.8116455078125\n",
            "Epoch: 719 - loss: 1597.3489990234375\n",
            "Epoch: 720 - loss: 1596.8865966796875\n",
            "Epoch: 721 - loss: 1596.4249267578125\n",
            "Epoch: 722 - loss: 1595.9638671875\n",
            "Epoch: 723 - loss: 1595.5035400390625\n",
            "Epoch: 724 - loss: 1595.0435791015625\n",
            "Epoch: 725 - loss: 1594.584716796875\n",
            "Epoch: 726 - loss: 1594.1256103515625\n",
            "Epoch: 727 - loss: 1593.6678466796875\n",
            "Epoch: 728 - loss: 1593.21044921875\n",
            "Epoch: 729 - loss: 1592.75390625\n",
            "Epoch: 730 - loss: 1592.2972412109375\n",
            "Epoch: 731 - loss: 1591.8414306640625\n",
            "Epoch: 732 - loss: 1591.3863525390625\n",
            "Epoch: 733 - loss: 1590.9322509765625\n",
            "Epoch: 734 - loss: 1590.4783935546875\n",
            "Epoch: 735 - loss: 1590.0247802734375\n",
            "Epoch: 736 - loss: 1589.5721435546875\n",
            "Epoch: 737 - loss: 1589.12060546875\n",
            "Epoch: 738 - loss: 1588.6689453125\n",
            "Epoch: 739 - loss: 1588.2181396484375\n",
            "Epoch: 740 - loss: 1587.7672119140625\n",
            "Epoch: 741 - loss: 1587.3177490234375\n",
            "Epoch: 742 - loss: 1586.8685302734375\n",
            "Epoch: 743 - loss: 1586.4197998046875\n",
            "Epoch: 744 - loss: 1585.9718017578125\n",
            "Epoch: 745 - loss: 1585.5244140625\n",
            "Epoch: 746 - loss: 1585.0771484375\n",
            "Epoch: 747 - loss: 1584.6312255859375\n",
            "Epoch: 748 - loss: 1584.1851806640625\n",
            "Epoch: 749 - loss: 1583.740478515625\n",
            "Epoch: 750 - loss: 1583.2958984375\n",
            "Epoch: 751 - loss: 1582.851806640625\n",
            "Epoch: 752 - loss: 1582.408447265625\n",
            "Epoch: 753 - loss: 1581.965576171875\n",
            "Epoch: 754 - loss: 1581.5233154296875\n",
            "Epoch: 755 - loss: 1581.08154296875\n",
            "Epoch: 756 - loss: 1580.6405029296875\n",
            "Epoch: 757 - loss: 1580.1995849609375\n",
            "Epoch: 758 - loss: 1579.759765625\n",
            "Epoch: 759 - loss: 1579.3203125\n",
            "Epoch: 760 - loss: 1578.881591796875\n",
            "Epoch: 761 - loss: 1578.443359375\n",
            "Epoch: 762 - loss: 1578.00537109375\n",
            "Epoch: 763 - loss: 1577.568603515625\n",
            "Epoch: 764 - loss: 1577.1319580078125\n",
            "Epoch: 765 - loss: 1576.695556640625\n",
            "Epoch: 766 - loss: 1576.260009765625\n",
            "Epoch: 767 - loss: 1575.8251953125\n",
            "Epoch: 768 - loss: 1575.390869140625\n",
            "Epoch: 769 - loss: 1574.957275390625\n",
            "Epoch: 770 - loss: 1574.5238037109375\n",
            "Epoch: 771 - loss: 1574.0908203125\n",
            "Epoch: 772 - loss: 1573.6588134765625\n",
            "Epoch: 773 - loss: 1573.2275390625\n",
            "Epoch: 774 - loss: 1572.79638671875\n",
            "Epoch: 775 - loss: 1572.365966796875\n",
            "Epoch: 776 - loss: 1571.935546875\n",
            "Epoch: 777 - loss: 1571.506591796875\n",
            "Epoch: 778 - loss: 1571.0777587890625\n",
            "Epoch: 779 - loss: 1570.6494140625\n",
            "Epoch: 780 - loss: 1570.2218017578125\n",
            "Epoch: 781 - loss: 1569.79443359375\n",
            "Epoch: 782 - loss: 1569.3681640625\n",
            "Epoch: 783 - loss: 1568.9420166015625\n",
            "Epoch: 784 - loss: 1568.516357421875\n",
            "Epoch: 785 - loss: 1568.0911865234375\n",
            "Epoch: 786 - loss: 1567.666748046875\n",
            "Epoch: 787 - loss: 1567.2425537109375\n",
            "Epoch: 788 - loss: 1566.8197021484375\n",
            "Epoch: 789 - loss: 1566.396728515625\n",
            "Epoch: 790 - loss: 1565.9744873046875\n",
            "Epoch: 791 - loss: 1565.552734375\n",
            "Epoch: 792 - loss: 1565.1312255859375\n",
            "Epoch: 793 - loss: 1564.7105712890625\n",
            "Epoch: 794 - loss: 1564.29052734375\n",
            "Epoch: 795 - loss: 1563.8707275390625\n",
            "Epoch: 796 - loss: 1563.4522705078125\n",
            "Epoch: 797 - loss: 1563.0335693359375\n",
            "Epoch: 798 - loss: 1562.6151123046875\n",
            "Epoch: 799 - loss: 1562.197998046875\n",
            "Epoch: 800 - loss: 1561.7808837890625\n",
            "Epoch: 801 - loss: 1561.3646240234375\n",
            "Epoch: 802 - loss: 1560.9482421875\n",
            "Epoch: 803 - loss: 1560.533447265625\n",
            "Epoch: 804 - loss: 1560.11865234375\n",
            "Epoch: 805 - loss: 1559.7042236328125\n",
            "Epoch: 806 - loss: 1559.2906494140625\n",
            "Epoch: 807 - loss: 1558.8773193359375\n",
            "Epoch: 808 - loss: 1558.4644775390625\n",
            "Epoch: 809 - loss: 1558.0526123046875\n",
            "Epoch: 810 - loss: 1557.6409912109375\n",
            "Epoch: 811 - loss: 1557.2298583984375\n",
            "Epoch: 812 - loss: 1556.819091796875\n",
            "Epoch: 813 - loss: 1556.408935546875\n",
            "Epoch: 814 - loss: 1555.9993896484375\n",
            "Epoch: 815 - loss: 1555.5902099609375\n",
            "Epoch: 816 - loss: 1555.181884765625\n",
            "Epoch: 817 - loss: 1554.7738037109375\n",
            "Epoch: 818 - loss: 1554.3662109375\n",
            "Epoch: 819 - loss: 1553.9593505859375\n",
            "Epoch: 820 - loss: 1553.552978515625\n",
            "Epoch: 821 - loss: 1553.1468505859375\n",
            "Epoch: 822 - loss: 1552.7413330078125\n",
            "Epoch: 823 - loss: 1552.3370361328125\n",
            "Epoch: 824 - loss: 1551.9326171875\n",
            "Epoch: 825 - loss: 1551.5283203125\n",
            "Epoch: 826 - loss: 1551.125\n",
            "Epoch: 827 - loss: 1550.7222900390625\n",
            "Epoch: 828 - loss: 1550.3194580078125\n",
            "Epoch: 829 - loss: 1549.9183349609375\n",
            "Epoch: 830 - loss: 1549.5167236328125\n",
            "Epoch: 831 - loss: 1549.115966796875\n",
            "Epoch: 832 - loss: 1548.7152099609375\n",
            "Epoch: 833 - loss: 1548.3155517578125\n",
            "Epoch: 834 - loss: 1547.916015625\n",
            "Epoch: 835 - loss: 1547.517578125\n",
            "Epoch: 836 - loss: 1547.119140625\n",
            "Epoch: 837 - loss: 1546.721435546875\n",
            "Epoch: 838 - loss: 1546.3248291015625\n",
            "Epoch: 839 - loss: 1545.92724609375\n",
            "Epoch: 840 - loss: 1545.53125\n",
            "Epoch: 841 - loss: 1545.1351318359375\n",
            "Epoch: 842 - loss: 1544.740234375\n",
            "Epoch: 843 - loss: 1544.3455810546875\n",
            "Epoch: 844 - loss: 1543.9510498046875\n",
            "Epoch: 845 - loss: 1543.557373046875\n",
            "Epoch: 846 - loss: 1543.1639404296875\n",
            "Epoch: 847 - loss: 1542.771484375\n",
            "Epoch: 848 - loss: 1542.3785400390625\n",
            "Epoch: 849 - loss: 1541.987060546875\n",
            "Epoch: 850 - loss: 1541.595703125\n",
            "Epoch: 851 - loss: 1541.2047119140625\n",
            "Epoch: 852 - loss: 1540.814453125\n",
            "Epoch: 853 - loss: 1540.4248046875\n",
            "Epoch: 854 - loss: 1540.0350341796875\n",
            "Epoch: 855 - loss: 1539.6461181640625\n",
            "Epoch: 856 - loss: 1539.2581787109375\n",
            "Epoch: 857 - loss: 1538.8702392578125\n",
            "Epoch: 858 - loss: 1538.4827880859375\n",
            "Epoch: 859 - loss: 1538.095703125\n",
            "Epoch: 860 - loss: 1537.7095947265625\n",
            "Epoch: 861 - loss: 1537.3236083984375\n",
            "Epoch: 862 - loss: 1536.9378662109375\n",
            "Epoch: 863 - loss: 1536.5531005859375\n",
            "Epoch: 864 - loss: 1536.1683349609375\n",
            "Epoch: 865 - loss: 1535.78466796875\n",
            "Epoch: 866 - loss: 1535.4010009765625\n",
            "Epoch: 867 - loss: 1535.017578125\n",
            "Epoch: 868 - loss: 1534.6351318359375\n",
            "Epoch: 869 - loss: 1534.2532958984375\n",
            "Epoch: 870 - loss: 1533.871337890625\n",
            "Epoch: 871 - loss: 1533.490234375\n",
            "Epoch: 872 - loss: 1533.109619140625\n",
            "Epoch: 873 - loss: 1532.7294921875\n",
            "Epoch: 874 - loss: 1532.349609375\n",
            "Epoch: 875 - loss: 1531.970703125\n",
            "Epoch: 876 - loss: 1531.591796875\n",
            "Epoch: 877 - loss: 1531.2132568359375\n",
            "Epoch: 878 - loss: 1530.8355712890625\n",
            "Epoch: 879 - loss: 1530.4583740234375\n",
            "Epoch: 880 - loss: 1530.0811767578125\n",
            "Epoch: 881 - loss: 1529.7049560546875\n",
            "Epoch: 882 - loss: 1529.3291015625\n",
            "Epoch: 883 - loss: 1528.9537353515625\n",
            "Epoch: 884 - loss: 1528.5787353515625\n",
            "Epoch: 885 - loss: 1528.2041015625\n",
            "Epoch: 886 - loss: 1527.830078125\n",
            "Epoch: 887 - loss: 1527.45654296875\n",
            "Epoch: 888 - loss: 1527.0833740234375\n",
            "Epoch: 889 - loss: 1526.7105712890625\n",
            "Epoch: 890 - loss: 1526.3388671875\n",
            "Epoch: 891 - loss: 1525.966796875\n",
            "Epoch: 892 - loss: 1525.5953369140625\n",
            "Epoch: 893 - loss: 1525.224609375\n",
            "Epoch: 894 - loss: 1524.854248046875\n",
            "Epoch: 895 - loss: 1524.484619140625\n",
            "Epoch: 896 - loss: 1524.1148681640625\n",
            "Epoch: 897 - loss: 1523.746337890625\n",
            "Epoch: 898 - loss: 1523.3773193359375\n",
            "Epoch: 899 - loss: 1523.0093994140625\n",
            "Epoch: 900 - loss: 1522.6419677734375\n",
            "Epoch: 901 - loss: 1522.27490234375\n",
            "Epoch: 902 - loss: 1521.9080810546875\n",
            "Epoch: 903 - loss: 1521.5416259765625\n",
            "Epoch: 904 - loss: 1521.176025390625\n",
            "Epoch: 905 - loss: 1520.810546875\n",
            "Epoch: 906 - loss: 1520.446044921875\n",
            "Epoch: 907 - loss: 1520.0814208984375\n",
            "Epoch: 908 - loss: 1519.7174072265625\n",
            "Epoch: 909 - loss: 1519.353759765625\n",
            "Epoch: 910 - loss: 1518.990966796875\n",
            "Epoch: 911 - loss: 1518.6282958984375\n",
            "Epoch: 912 - loss: 1518.2659912109375\n",
            "Epoch: 913 - loss: 1517.9041748046875\n",
            "Epoch: 914 - loss: 1517.543212890625\n",
            "Epoch: 915 - loss: 1517.1820068359375\n",
            "Epoch: 916 - loss: 1516.82177734375\n",
            "Epoch: 917 - loss: 1516.461669921875\n",
            "Epoch: 918 - loss: 1516.102294921875\n",
            "Epoch: 919 - loss: 1515.7432861328125\n",
            "Epoch: 920 - loss: 1515.3846435546875\n",
            "Epoch: 921 - loss: 1515.0264892578125\n",
            "Epoch: 922 - loss: 1514.6689453125\n",
            "Epoch: 923 - loss: 1514.3115234375\n",
            "Epoch: 924 - loss: 1513.95458984375\n",
            "Epoch: 925 - loss: 1513.5982666015625\n",
            "Epoch: 926 - loss: 1513.242431640625\n",
            "Epoch: 927 - loss: 1512.8870849609375\n",
            "Epoch: 928 - loss: 1512.531494140625\n",
            "Epoch: 929 - loss: 1512.1768798828125\n",
            "Epoch: 930 - loss: 1511.8228759765625\n",
            "Epoch: 931 - loss: 1511.46875\n",
            "Epoch: 932 - loss: 1511.115478515625\n",
            "Epoch: 933 - loss: 1510.7628173828125\n",
            "Epoch: 934 - loss: 1510.410400390625\n",
            "Epoch: 935 - loss: 1510.0584716796875\n",
            "Epoch: 936 - loss: 1509.7066650390625\n",
            "Epoch: 937 - loss: 1509.3553466796875\n",
            "Epoch: 938 - loss: 1509.004638671875\n",
            "Epoch: 939 - loss: 1508.654541015625\n",
            "Epoch: 940 - loss: 1508.3045654296875\n",
            "Epoch: 941 - loss: 1507.955078125\n",
            "Epoch: 942 - loss: 1507.6064453125\n",
            "Epoch: 943 - loss: 1507.25732421875\n",
            "Epoch: 944 - loss: 1506.9095458984375\n",
            "Epoch: 945 - loss: 1506.5616455078125\n",
            "Epoch: 946 - loss: 1506.2144775390625\n",
            "Epoch: 947 - loss: 1505.8675537109375\n",
            "Epoch: 948 - loss: 1505.5208740234375\n",
            "Epoch: 949 - loss: 1505.1748046875\n",
            "Epoch: 950 - loss: 1504.8294677734375\n",
            "Epoch: 951 - loss: 1504.4840087890625\n",
            "Epoch: 952 - loss: 1504.1396484375\n",
            "Epoch: 953 - loss: 1503.795166015625\n",
            "Epoch: 954 - loss: 1503.451171875\n",
            "Epoch: 955 - loss: 1503.1077880859375\n",
            "Epoch: 956 - loss: 1502.764404296875\n",
            "Epoch: 957 - loss: 1502.4217529296875\n",
            "Epoch: 958 - loss: 1502.0794677734375\n",
            "Epoch: 959 - loss: 1501.73779296875\n",
            "Epoch: 960 - loss: 1501.3963623046875\n",
            "Epoch: 961 - loss: 1501.0552978515625\n",
            "Epoch: 962 - loss: 1500.7147216796875\n",
            "Epoch: 963 - loss: 1500.37451171875\n",
            "Epoch: 964 - loss: 1500.03515625\n",
            "Epoch: 965 - loss: 1499.695556640625\n",
            "Epoch: 966 - loss: 1499.3565673828125\n",
            "Epoch: 967 - loss: 1499.018310546875\n",
            "Epoch: 968 - loss: 1498.680419921875\n",
            "Epoch: 969 - loss: 1498.3427734375\n",
            "Epoch: 970 - loss: 1498.0052490234375\n",
            "Epoch: 971 - loss: 1497.6683349609375\n",
            "Epoch: 972 - loss: 1497.33203125\n",
            "Epoch: 973 - loss: 1496.9959716796875\n",
            "Epoch: 974 - loss: 1496.660400390625\n",
            "Epoch: 975 - loss: 1496.324951171875\n",
            "Epoch: 976 - loss: 1495.990478515625\n",
            "Epoch: 977 - loss: 1495.65576171875\n",
            "Epoch: 978 - loss: 1495.3218994140625\n",
            "Epoch: 979 - loss: 1494.9879150390625\n",
            "Epoch: 980 - loss: 1494.655029296875\n",
            "Epoch: 981 - loss: 1494.322509765625\n",
            "Epoch: 982 - loss: 1493.9898681640625\n",
            "Epoch: 983 - loss: 1493.6578369140625\n",
            "Epoch: 984 - loss: 1493.3265380859375\n",
            "Epoch: 985 - loss: 1492.9951171875\n",
            "Epoch: 986 - loss: 1492.664306640625\n",
            "Epoch: 987 - loss: 1492.333984375\n",
            "Epoch: 988 - loss: 1492.004150390625\n",
            "Epoch: 989 - loss: 1491.6744384765625\n",
            "Epoch: 990 - loss: 1491.3455810546875\n",
            "Epoch: 991 - loss: 1491.0167236328125\n",
            "Epoch: 992 - loss: 1490.6881103515625\n",
            "Epoch: 993 - loss: 1490.3603515625\n",
            "Epoch: 994 - loss: 1490.03271484375\n",
            "Epoch: 995 - loss: 1489.7056884765625\n",
            "Epoch: 996 - loss: 1489.379150390625\n",
            "Epoch: 997 - loss: 1489.0526123046875\n",
            "Epoch: 998 - loss: 1488.7265625\n",
            "Epoch: 999 - loss: 1488.4007568359375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1f5IyLf_3hD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "7d314d97-fc19-4449-9243-70442141cb3b"
      },
      "source": [
        "# Plotting the loss\n",
        "\n",
        "figure = go.Figure(\n",
        "    data = go.Scatter(\n",
        "        x=Loss\n",
        "    )\n",
        ")\n",
        "\n",
        "figure.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"01521f7e-11a0-4589-a3c0-e52a744d9beb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"01521f7e-11a0-4589-a3c0-e52a744d9beb\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '01521f7e-11a0-4589-a3c0-e52a744d9beb',\n",
              "                        [{\"type\": \"scatter\", \"x\": [2166.2802734375, 2165.0078125, 2163.7373046875, 2162.468017578125, 2161.201171875, 2159.935546875, 2158.672119140625, 2157.41015625, 2156.15087890625, 2154.893798828125, 2153.6376953125, 2152.384033203125, 2151.132080078125, 2149.88232421875, 2148.634033203125, 2147.3876953125, 2146.14306640625, 2144.900390625, 2143.659912109375, 2142.421142578125, 2141.184326171875, 2139.948974609375, 2138.715576171875, 2137.484375, 2136.25439453125, 2135.026611328125, 2133.80126953125, 2132.577392578125, 2131.35546875, 2130.13427734375, 2128.916259765625, 2127.69921875, 2126.48388671875, 2125.2705078125, 2124.059814453125, 2122.849853515625, 2121.642578125, 2120.436279296875, 2119.232421875, 2118.030517578125, 2116.829833984375, 2115.630859375, 2114.43408203125, 2113.239501953125, 2112.04541015625, 2110.854248046875, 2109.664306640625, 2108.4765625, 2107.2900390625, 2106.10595703125, 2104.923095703125, 2103.74267578125, 2102.56298828125, 2101.385986328125, 2100.21044921875, 2099.036376953125, 2097.864501953125, 2096.694091796875, 2095.525390625, 2094.358642578125, 2093.193603515625, 2092.02978515625, 2090.868408203125, 2089.708984375, 2088.550048828125, 2087.3935546875, 2086.239013671875, 2085.0859375, 2083.93408203125, 2082.78515625, 2081.63720703125, 2080.4912109375, 2079.3466796875, 2078.203369140625, 2077.0625, 2075.923095703125, 2074.785400390625, 2073.649169921875, 2072.514892578125, 2071.3828125, 2070.251220703125, 2069.122802734375, 2067.994873046875, 2066.86865234375, 2065.74462890625, 2064.622314453125, 2063.5009765625, 2062.382080078125, 2061.26416015625, 2060.148193359375, 2059.03369140625, 2057.921630859375, 2056.809814453125, 2055.701416015625, 2054.593017578125, 2053.487548828125, 2052.3828125, 2051.27978515625, 2050.178955078125, 2049.080078125, 2047.9810791015625, 2046.8851318359375, 2045.7904052734375, 2044.697265625, 2043.6065673828125, 2042.516357421875, 2041.4283447265625, 2040.3421630859375, 2039.2572021484375, 2038.173583984375, 2037.0916748046875, 2036.01123046875, 2034.9329833984375, 2033.8564453125, 2032.7806396484375, 2031.70703125, 2030.634765625, 2029.56396484375, 2028.4951171875, 2027.427978515625, 2026.3616943359375, 2025.2972412109375, 2024.2347412109375, 2023.173583984375, 2022.1136474609375, 2021.0557861328125, 2019.9991455078125, 2018.9447021484375, 2017.8905029296875, 2016.8389892578125, 2015.7889404296875, 2014.7406005859375, 2013.69287109375, 2012.6474609375, 2011.603515625, 2010.560791015625, 2009.5198974609375, 2008.480224609375, 2007.4417724609375, 2006.4052734375, 2005.3709716796875, 2004.33740234375, 2003.3056640625, 2002.275390625, 2001.246337890625, 2000.2191162109375, 1999.19287109375, 1998.1693115234375, 1997.146240234375, 1996.124755859375, 1995.1048583984375, 1994.0869140625, 1993.0697021484375, 1992.054443359375, 1991.041259765625, 1990.028076171875, 1989.01708984375, 1988.0078125, 1987.000244140625, 1985.99365234375, 1984.98828125, 1983.9849853515625, 1982.9827880859375, 1981.982421875, 1980.9827880859375, 1979.986083984375, 1978.98974609375, 1977.9952392578125, 1977.001708984375, 1976.0101318359375, 1975.01953125, 1974.0306396484375, 1973.0435791015625, 1972.0570068359375, 1971.0728759765625, 1970.0897216796875, 1969.1077880859375, 1968.1279296875, 1967.148681640625, 1966.171630859375, 1965.1953125, 1964.220703125, 1963.247802734375, 1962.2760009765625, 1961.3056640625, 1960.3370361328125, 1959.369140625, 1958.4034423828125, 1957.4388427734375, 1956.4757080078125, 1955.5135498046875, 1954.5531005859375, 1953.5941162109375, 1952.6370849609375, 1951.6806640625, 1950.7252197265625, 1949.7720947265625, 1948.8201904296875, 1947.8697509765625, 1946.919677734375, 1945.9718017578125, 1945.0252685546875, 1944.080078125, 1943.1357421875, 1942.1943359375, 1941.2530517578125, 1940.3128662109375, 1939.3753662109375, 1938.4384765625, 1937.5023193359375, 1936.5682373046875, 1935.635498046875, 1934.7037353515625, 1933.7733154296875, 1932.8450927734375, 1931.9173583984375, 1930.9912109375, 1930.06640625, 1929.1431884765625, 1928.2210693359375, 1927.3001708984375, 1926.38037109375, 1925.462646484375, 1924.545654296875, 1923.6307373046875, 1922.716552734375, 1921.803466796875, 1920.8924560546875, 1919.9822998046875, 1919.0732421875, 1918.1654052734375, 1917.259521484375, 1916.3546142578125, 1915.450927734375, 1914.548828125, 1913.6478271484375, 1912.7479248046875, 1911.8494873046875, 1910.9525146484375, 1910.056640625, 1909.1619873046875, 1908.2689208984375, 1907.3773193359375, 1906.486328125, 1905.59716796875, 1904.708740234375, 1903.8221435546875, 1902.937255859375, 1902.052490234375, 1901.16943359375, 1900.2882080078125, 1899.4075927734375, 1898.528076171875, 1897.650634765625, 1896.7740478515625, 1895.8984375, 1895.0245361328125, 1894.15185546875, 1893.280029296875, 1892.410400390625, 1891.5408935546875, 1890.6729736328125, 1889.806640625, 1888.9412841796875, 1888.0771484375, 1887.2142333984375, 1886.3529052734375, 1885.4925537109375, 1884.6334228515625, 1883.7752685546875, 1882.9185791015625, 1882.063232421875, 1881.2088623046875, 1880.356201171875, 1879.5045166015625, 1878.6541748046875, 1877.804443359375, 1876.95654296875, 1876.1092529296875, 1875.263427734375, 1874.41943359375, 1873.576171875, 1872.734130859375, 1871.8935546875, 1871.0538330078125, 1870.21484375, 1869.3782958984375, 1868.541748046875, 1867.70703125, 1866.8736572265625, 1866.0413818359375, 1865.2103271484375, 1864.3802490234375, 1863.551025390625, 1862.7236328125, 1861.896728515625, 1861.072021484375, 1860.247802734375, 1859.424560546875, 1858.6029052734375, 1857.7822265625, 1856.963134765625, 1856.144775390625, 1855.3275146484375, 1854.511474609375, 1853.6966552734375, 1852.8831787109375, 1852.0706787109375, 1851.2596435546875, 1850.448974609375, 1849.6405029296875, 1848.83203125, 1848.0260009765625, 1847.22021484375, 1846.4154052734375, 1845.61279296875, 1844.8104248046875, 1844.0096435546875, 1843.209716796875, 1842.4111328125, 1841.613525390625, 1840.8175048828125, 1840.0220947265625, 1839.227294921875, 1838.4349365234375, 1837.642578125, 1836.8519287109375, 1836.0623779296875, 1835.273681640625, 1834.486083984375, 1833.7003173828125, 1832.914794921875, 1832.131103515625, 1831.3480224609375, 1830.56640625, 1829.784912109375, 1829.006103515625, 1828.227294921875, 1827.4501953125, 1826.6732177734375, 1825.898193359375, 1825.1243896484375, 1824.35107421875, 1823.5794677734375, 1822.80859375, 1822.039306640625, 1821.2698974609375, 1820.5025634765625, 1819.7362060546875, 1818.970947265625, 1818.20654296875, 1817.44287109375, 1816.6810302734375, 1815.919921875, 1815.15966796875, 1814.4010009765625, 1813.6435546875, 1812.88671875, 1812.1307373046875, 1811.3763427734375, 1810.6224365234375, 1809.869873046875, 1809.1182861328125, 1808.367919921875, 1807.61865234375, 1806.8701171875, 1806.123291015625, 1805.3768310546875, 1804.6322021484375, 1803.8876953125, 1803.1444091796875, 1802.4033203125, 1801.662109375, 1800.9217529296875, 1800.1829833984375, 1799.4453125, 1798.7081298828125, 1797.97265625, 1797.23779296875, 1796.5042724609375, 1795.771728515625, 1795.0401611328125, 1794.309326171875, 1793.579833984375, 1792.851318359375, 1792.1236572265625, 1791.3978271484375, 1790.671875, 1789.947998046875, 1789.22412109375, 1788.501708984375, 1787.780029296875, 1787.0596923828125, 1786.3408203125, 1785.621826171875, 1784.9049072265625, 1784.1881103515625, 1783.47265625, 1782.758544921875, 1782.045166015625, 1781.3330078125, 1780.6217041015625, 1779.9112548828125, 1779.201904296875, 1778.4932861328125, 1777.785888671875, 1777.0794677734375, 1776.3740234375, 1775.669921875, 1774.96630859375, 1774.2635498046875, 1773.5623779296875, 1772.862060546875, 1772.1619873046875, 1771.463623046875, 1770.7659912109375, 1770.0693359375, 1769.373779296875, 1768.67919921875, 1767.9847412109375, 1767.29248046875, 1766.6009521484375, 1765.91015625, 1765.22021484375, 1764.531494140625, 1763.843505859375, 1763.156494140625, 1762.4705810546875, 1761.7850341796875, 1761.1014404296875, 1760.418212890625, 1759.736083984375, 1759.054443359375, 1758.3741455078125, 1757.6947021484375, 1757.0167236328125, 1756.3392333984375, 1755.6627197265625, 1754.9869384765625, 1754.3123779296875, 1753.638916015625, 1752.9659423828125, 1752.29443359375, 1751.62255859375, 1750.95263671875, 1750.2835693359375, 1749.6156005859375, 1748.94873046875, 1748.2823486328125, 1747.61669921875, 1746.95263671875, 1746.28857421875, 1745.6263427734375, 1744.96435546875, 1744.3037109375, 1743.644287109375, 1742.9849853515625, 1742.3265380859375, 1741.669921875, 1741.0135498046875, 1740.3583984375, 1739.703857421875, 1739.050537109375, 1738.3975830078125, 1737.74609375, 1737.095458984375, 1736.4451904296875, 1735.79638671875, 1735.1483154296875, 1734.5013427734375, 1733.8546142578125, 1733.209228515625, 1732.5643310546875, 1731.9212646484375, 1731.2783203125, 1730.63623046875, 1729.9954833984375, 1729.3553466796875, 1728.7159423828125, 1728.07763671875, 1727.440185546875, 1726.8038330078125, 1726.16796875, 1725.5330810546875, 1724.899169921875, 1724.2662353515625, 1723.6339111328125, 1723.0023193359375, 1722.3720703125, 1721.741943359375, 1721.1136474609375, 1720.48583984375, 1719.8587646484375, 1719.232177734375, 1718.6068115234375, 1717.9830322265625, 1717.359130859375, 1716.736083984375, 1716.114013671875, 1715.4931640625, 1714.873046875, 1714.253662109375, 1713.635009765625, 1713.017822265625, 1712.400390625, 1711.7845458984375, 1711.169921875, 1710.5552978515625, 1709.9417724609375, 1709.3292236328125, 1708.717529296875, 1708.1065673828125, 1707.4964599609375, 1706.8873291015625, 1706.279052734375, 1705.6712646484375, 1705.0643310546875, 1704.45849609375, 1703.853271484375, 1703.2490234375, 1702.6458740234375, 1702.042724609375, 1701.44091796875, 1700.8404541015625, 1700.23974609375, 1699.6405029296875, 1699.0419921875, 1698.444091796875, 1697.84765625, 1697.2509765625, 1696.656005859375, 1696.061279296875, 1695.467529296875, 1694.874755859375, 1694.2830810546875, 1693.6917724609375, 1693.1009521484375, 1692.511474609375, 1691.9224853515625, 1691.334716796875, 1690.746826171875, 1690.1607666015625, 1689.5751953125, 1688.990234375, 1688.4061279296875, 1687.8228759765625, 1687.240478515625, 1686.658935546875, 1686.0780029296875, 1685.497802734375, 1684.91796875, 1684.3397216796875, 1683.761962890625, 1683.1846923828125, 1682.609130859375, 1682.0330810546875, 1681.45849609375, 1680.884765625, 1680.3116455078125, 1679.7392578125, 1679.167724609375, 1678.5968017578125, 1678.027099609375, 1677.457763671875, 1676.8892822265625, 1676.32177734375, 1675.754638671875, 1675.1885986328125, 1674.6234130859375, 1674.0582275390625, 1673.4947509765625, 1672.9315185546875, 1672.3687744140625, 1671.8076171875, 1671.2467041015625, 1670.686279296875, 1670.127197265625, 1669.568359375, 1669.010498046875, 1668.4534912109375, 1667.8974609375, 1667.342041015625, 1666.7867431640625, 1666.2327880859375, 1665.6795654296875, 1665.1265869140625, 1664.5745849609375, 1664.0240478515625, 1663.4739990234375, 1662.924072265625, 1662.3748779296875, 1661.8265380859375, 1661.2789306640625, 1660.732421875, 1660.1866455078125, 1659.6409912109375, 1659.0968017578125, 1658.5533447265625, 1658.0103759765625, 1657.4677734375, 1656.926025390625, 1656.3853759765625, 1655.84521484375, 1655.3056640625, 1654.7667236328125, 1654.229248046875, 1653.691650390625, 1653.1552734375, 1652.619384765625, 1652.084228515625, 1651.5499267578125, 1651.0162353515625, 1650.4835205078125, 1649.9508056640625, 1649.4190673828125, 1648.888671875, 1648.3583984375, 1647.8291015625, 1647.30029296875, 1646.7725830078125, 1646.244873046875, 1645.7183837890625, 1645.1927490234375, 1644.6673583984375, 1644.1429443359375, 1643.6187744140625, 1643.0963134765625, 1642.5733642578125, 1642.051513671875, 1641.53076171875, 1641.010498046875, 1640.4908447265625, 1639.97216796875, 1639.4537353515625, 1638.9359130859375, 1638.4190673828125, 1637.9027099609375, 1637.387451171875, 1636.8724365234375, 1636.3585205078125, 1635.8450927734375, 1635.33203125, 1634.81982421875, 1634.3089599609375, 1633.7979736328125, 1633.287841796875, 1632.7786865234375, 1632.269775390625, 1631.7615966796875, 1631.2545166015625, 1630.7479248046875, 1630.24169921875, 1629.7359619140625, 1629.2315673828125, 1628.7276611328125, 1628.22412109375, 1627.721435546875, 1627.219482421875, 1626.7181396484375, 1626.217529296875, 1625.717529296875, 1625.21826171875, 1624.719482421875, 1624.2213134765625, 1623.7237548828125, 1623.2271728515625, 1622.730712890625, 1622.2353515625, 1621.7406005859375, 1621.246826171875, 1620.7529296875, 1620.2603759765625, 1619.767578125, 1619.2764892578125, 1618.7855224609375, 1618.295166015625, 1617.8057861328125, 1617.316650390625, 1616.828369140625, 1616.3409423828125, 1615.8538818359375, 1615.3671875, 1614.881591796875, 1614.396484375, 1613.912353515625, 1613.4281005859375, 1612.94482421875, 1612.46240234375, 1611.98046875, 1611.4993896484375, 1611.0185546875, 1610.5384521484375, 1610.0589599609375, 1609.580322265625, 1609.1021728515625, 1608.6243896484375, 1608.1475830078125, 1607.6712646484375, 1607.195556640625, 1606.7203369140625, 1606.2459716796875, 1605.772216796875, 1605.2991943359375, 1604.8265380859375, 1604.3544921875, 1603.8834228515625, 1603.4127197265625, 1602.9423828125, 1602.472900390625, 1602.0037841796875, 1601.5357666015625, 1601.068359375, 1600.6009521484375, 1600.1343994140625, 1599.6690673828125, 1599.2037353515625, 1598.7388916015625, 1598.2750244140625, 1597.8116455078125, 1597.3489990234375, 1596.8865966796875, 1596.4249267578125, 1595.9638671875, 1595.5035400390625, 1595.0435791015625, 1594.584716796875, 1594.1256103515625, 1593.6678466796875, 1593.21044921875, 1592.75390625, 1592.2972412109375, 1591.8414306640625, 1591.3863525390625, 1590.9322509765625, 1590.4783935546875, 1590.0247802734375, 1589.5721435546875, 1589.12060546875, 1588.6689453125, 1588.2181396484375, 1587.7672119140625, 1587.3177490234375, 1586.8685302734375, 1586.4197998046875, 1585.9718017578125, 1585.5244140625, 1585.0771484375, 1584.6312255859375, 1584.1851806640625, 1583.740478515625, 1583.2958984375, 1582.851806640625, 1582.408447265625, 1581.965576171875, 1581.5233154296875, 1581.08154296875, 1580.6405029296875, 1580.1995849609375, 1579.759765625, 1579.3203125, 1578.881591796875, 1578.443359375, 1578.00537109375, 1577.568603515625, 1577.1319580078125, 1576.695556640625, 1576.260009765625, 1575.8251953125, 1575.390869140625, 1574.957275390625, 1574.5238037109375, 1574.0908203125, 1573.6588134765625, 1573.2275390625, 1572.79638671875, 1572.365966796875, 1571.935546875, 1571.506591796875, 1571.0777587890625, 1570.6494140625, 1570.2218017578125, 1569.79443359375, 1569.3681640625, 1568.9420166015625, 1568.516357421875, 1568.0911865234375, 1567.666748046875, 1567.2425537109375, 1566.8197021484375, 1566.396728515625, 1565.9744873046875, 1565.552734375, 1565.1312255859375, 1564.7105712890625, 1564.29052734375, 1563.8707275390625, 1563.4522705078125, 1563.0335693359375, 1562.6151123046875, 1562.197998046875, 1561.7808837890625, 1561.3646240234375, 1560.9482421875, 1560.533447265625, 1560.11865234375, 1559.7042236328125, 1559.2906494140625, 1558.8773193359375, 1558.4644775390625, 1558.0526123046875, 1557.6409912109375, 1557.2298583984375, 1556.819091796875, 1556.408935546875, 1555.9993896484375, 1555.5902099609375, 1555.181884765625, 1554.7738037109375, 1554.3662109375, 1553.9593505859375, 1553.552978515625, 1553.1468505859375, 1552.7413330078125, 1552.3370361328125, 1551.9326171875, 1551.5283203125, 1551.125, 1550.7222900390625, 1550.3194580078125, 1549.9183349609375, 1549.5167236328125, 1549.115966796875, 1548.7152099609375, 1548.3155517578125, 1547.916015625, 1547.517578125, 1547.119140625, 1546.721435546875, 1546.3248291015625, 1545.92724609375, 1545.53125, 1545.1351318359375, 1544.740234375, 1544.3455810546875, 1543.9510498046875, 1543.557373046875, 1543.1639404296875, 1542.771484375, 1542.3785400390625, 1541.987060546875, 1541.595703125, 1541.2047119140625, 1540.814453125, 1540.4248046875, 1540.0350341796875, 1539.6461181640625, 1539.2581787109375, 1538.8702392578125, 1538.4827880859375, 1538.095703125, 1537.7095947265625, 1537.3236083984375, 1536.9378662109375, 1536.5531005859375, 1536.1683349609375, 1535.78466796875, 1535.4010009765625, 1535.017578125, 1534.6351318359375, 1534.2532958984375, 1533.871337890625, 1533.490234375, 1533.109619140625, 1532.7294921875, 1532.349609375, 1531.970703125, 1531.591796875, 1531.2132568359375, 1530.8355712890625, 1530.4583740234375, 1530.0811767578125, 1529.7049560546875, 1529.3291015625, 1528.9537353515625, 1528.5787353515625, 1528.2041015625, 1527.830078125, 1527.45654296875, 1527.0833740234375, 1526.7105712890625, 1526.3388671875, 1525.966796875, 1525.5953369140625, 1525.224609375, 1524.854248046875, 1524.484619140625, 1524.1148681640625, 1523.746337890625, 1523.3773193359375, 1523.0093994140625, 1522.6419677734375, 1522.27490234375, 1521.9080810546875, 1521.5416259765625, 1521.176025390625, 1520.810546875, 1520.446044921875, 1520.0814208984375, 1519.7174072265625, 1519.353759765625, 1518.990966796875, 1518.6282958984375, 1518.2659912109375, 1517.9041748046875, 1517.543212890625, 1517.1820068359375, 1516.82177734375, 1516.461669921875, 1516.102294921875, 1515.7432861328125, 1515.3846435546875, 1515.0264892578125, 1514.6689453125, 1514.3115234375, 1513.95458984375, 1513.5982666015625, 1513.242431640625, 1512.8870849609375, 1512.531494140625, 1512.1768798828125, 1511.8228759765625, 1511.46875, 1511.115478515625, 1510.7628173828125, 1510.410400390625, 1510.0584716796875, 1509.7066650390625, 1509.3553466796875, 1509.004638671875, 1508.654541015625, 1508.3045654296875, 1507.955078125, 1507.6064453125, 1507.25732421875, 1506.9095458984375, 1506.5616455078125, 1506.2144775390625, 1505.8675537109375, 1505.5208740234375, 1505.1748046875, 1504.8294677734375, 1504.4840087890625, 1504.1396484375, 1503.795166015625, 1503.451171875, 1503.1077880859375, 1502.764404296875, 1502.4217529296875, 1502.0794677734375, 1501.73779296875, 1501.3963623046875, 1501.0552978515625, 1500.7147216796875, 1500.37451171875, 1500.03515625, 1499.695556640625, 1499.3565673828125, 1499.018310546875, 1498.680419921875, 1498.3427734375, 1498.0052490234375, 1497.6683349609375, 1497.33203125, 1496.9959716796875, 1496.660400390625, 1496.324951171875, 1495.990478515625, 1495.65576171875, 1495.3218994140625, 1494.9879150390625, 1494.655029296875, 1494.322509765625, 1493.9898681640625, 1493.6578369140625, 1493.3265380859375, 1492.9951171875, 1492.664306640625, 1492.333984375, 1492.004150390625, 1491.6744384765625, 1491.3455810546875, 1491.0167236328125, 1490.6881103515625, 1490.3603515625, 1490.03271484375, 1489.7056884765625, 1489.379150390625, 1489.0526123046875, 1488.7265625, 1488.4007568359375]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('01521f7e-11a0-4589-a3c0-e52a744d9beb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOIv876DCHoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd08a53b-1f83-4121-df91-8a3561b1b553"
      },
      "source": [
        "y_pred_test_tensor = lr.pred(X_test_tensor)\n",
        "Loss_pred = lr.loss(y_test_tensor, y_pred_tensor)\n",
        "Loss_pred\n",
        "# figure = go.Figure(\n",
        "#     data = go.Scatter(\n",
        "#         x=Loss_pred\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# figure.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1869.9482, grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARVlaDMbwlgH",
        "colab_type": "text"
      },
      "source": [
        "# Classification problem"
      ]
    }
  ]
}