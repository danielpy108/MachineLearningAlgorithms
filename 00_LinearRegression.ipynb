{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00-LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGTJo6BulvTcG4AOXyjUZF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielpy108/MachineLearningAlgorithms/blob/master/00_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTinNrY2kQ6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaTl2grmwhiQ",
        "colab_type": "text"
      },
      "source": [
        "# Regression problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz9IfMYLwpl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_diabetes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrr7vPRtkaZC",
        "colab_type": "code",
        "outputId": "bdbb48a5-d5e1-4b89-dab7-96f23cee86aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "training_data = load_diabetes()\n",
        "\n",
        "X = training_data['data']\n",
        "y = training_data['target']\n",
        "features = training_data['feature_names']\n",
        "description = training_data['DESCR']\n",
        "\n",
        "print(f'Data samples: {X.shape[0]}')\n",
        "print(f'Input features (dimenssion): {X.shape[1]}')\n",
        "print(f\"Features: {features}\")\n",
        "print(description)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data samples: 442\n",
            "Input features (dimenssion): 10\n",
            "Features: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
            ".. _diabetes_dataset:\n",
            "\n",
            "Diabetes dataset\n",
            "----------------\n",
            "\n",
            "Ten baseline variables, age, sex, body mass index, average blood\n",
            "pressure, and six blood serum measurements were obtained for each of n =\n",
            "442 diabetes patients, as well as the response of interest, a\n",
            "quantitative measure of disease progression one year after baseline.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "  :Number of Instances: 442\n",
            "\n",
            "  :Number of Attributes: First 10 columns are numeric predictive values\n",
            "\n",
            "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
            "\n",
            "  :Attribute Information:\n",
            "      - Age\n",
            "      - Sex\n",
            "      - Body mass index\n",
            "      - Average blood pressure\n",
            "      - S1\n",
            "      - S2\n",
            "      - S3\n",
            "      - S4\n",
            "      - S5\n",
            "      - S6\n",
            "\n",
            "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
            "\n",
            "Source URL:\n",
            "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
            "\n",
            "For more information see:\n",
            "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
            "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziBd_fYCzAmi",
        "colab_type": "code",
        "outputId": "1179f5ff-ab14-48bf-869c-7dd26f0a60f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Turn the data into a pandas DataFrame\n",
        "df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=features+['Output'])\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "      <th>Output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019908</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>151.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068330</td>\n",
              "      <td>-0.092204</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>-0.025930</td>\n",
              "      <td>141.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.009362</td>\n",
              "      <td>206.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.046641</td>\n",
              "      <td>135.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        age       sex       bmi        bp  ...        s4        s5        s6  Output\n",
              "0  0.038076  0.050680  0.061696  0.021872  ... -0.002592  0.019908 -0.017646   151.0\n",
              "1 -0.001882 -0.044642 -0.051474 -0.026328  ... -0.039493 -0.068330 -0.092204    75.0\n",
              "2  0.085299  0.050680  0.044451 -0.005671  ... -0.002592  0.002864 -0.025930   141.0\n",
              "3 -0.089063 -0.044642 -0.011595 -0.036656  ...  0.034309  0.022692 -0.009362   206.0\n",
              "4  0.005383 -0.044642 -0.036385  0.021872  ... -0.002592 -0.031991 -0.046641   135.0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99AvSGRI4m4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to separate the dataset into training and test samples\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, train_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIqHvzGz23Zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, the data will be turned into a pytorch tensor for the linear\n",
        "# regression model\n",
        "X_training_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_training_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FFTahG58eD",
        "colab_type": "text"
      },
      "source": [
        "## Creating the linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N4QBT4V5_IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearRegression():\n",
        "    def __init__(self, nsamples, nfeatures):\n",
        "        self.W = torch.randn(nfeatures, requires_grad=True)\n",
        "        self.b = torch.randn(1, requires_grad=True)\n",
        "    \n",
        "    def pred(self, x):\n",
        "        return x@self.W + self.b\n",
        "    \n",
        "    def loss(self, y, y_pred):\n",
        "        squared_error = (y - y_pred).pow(2)\n",
        "        N = torch.numel(squared_error)\n",
        "        return 1/(2*N) * torch.sum(squared_error)\n",
        "    \n",
        "    def fit(self, X, y, lr, epochs):\n",
        "        Loss = []\n",
        "        for e in range(epochs):\n",
        "            y_pred = self.pred(X)\n",
        "            loss = self.loss(y, y_pred)\n",
        "            Loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                self.W -=  lr*self.W.grad\n",
        "                self.b -= lr*self.b.grad\n",
        "                self.W.grad.zero_()\n",
        "                self.b.grad.zero_()\n",
        "            if (e+1)%10 == 0:\n",
        "                print(f\"Epoch [{e}/{epochs}] - Loss: {loss}\")\n",
        "        return np.array(Loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiW1qJhm63LR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "\n",
        "nsamples = X_training_tensor.shape[0]\n",
        "nfeatures = X_training_tensor.shape[1]\n",
        "\n",
        "lr = LinearRegression(nsamples, nfeatures)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGtdsG01-qv4",
        "colab_type": "code",
        "outputId": "1d6c2586-182a-494e-c668-9f675447c239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Loss = lr.fit(X_training_tensor, y_training_tensor, lr=0.1, epochs=1000)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [9/1000] - Loss: 4083.1005859375\n",
            "Epoch [19/1000] - Loss: 2639.25\n",
            "Epoch [29/1000] - Loss: 2445.451904296875\n",
            "Epoch [39/1000] - Loss: 2403.946533203125\n",
            "Epoch [49/1000] - Loss: 2381.29052734375\n",
            "Epoch [59/1000] - Loss: 2361.2607421875\n",
            "Epoch [69/1000] - Loss: 2341.875244140625\n",
            "Epoch [79/1000] - Loss: 2322.887939453125\n",
            "Epoch [89/1000] - Loss: 2304.260986328125\n",
            "Epoch [99/1000] - Loss: 2285.98583984375\n",
            "Epoch [109/1000] - Loss: 2268.05322265625\n",
            "Epoch [119/1000] - Loss: 2250.455810546875\n",
            "Epoch [129/1000] - Loss: 2233.188232421875\n",
            "Epoch [139/1000] - Loss: 2216.241943359375\n",
            "Epoch [149/1000] - Loss: 2199.61083984375\n",
            "Epoch [159/1000] - Loss: 2183.2890625\n",
            "Epoch [169/1000] - Loss: 2167.27001953125\n",
            "Epoch [179/1000] - Loss: 2151.54736328125\n",
            "Epoch [189/1000] - Loss: 2136.115478515625\n",
            "Epoch [199/1000] - Loss: 2120.968017578125\n",
            "Epoch [209/1000] - Loss: 2106.098876953125\n",
            "Epoch [219/1000] - Loss: 2091.503173828125\n",
            "Epoch [229/1000] - Loss: 2077.17431640625\n",
            "Epoch [239/1000] - Loss: 2063.108642578125\n",
            "Epoch [249/1000] - Loss: 2049.29833984375\n",
            "Epoch [259/1000] - Loss: 2035.7401123046875\n",
            "Epoch [269/1000] - Loss: 2022.427734375\n",
            "Epoch [279/1000] - Loss: 2009.3568115234375\n",
            "Epoch [289/1000] - Loss: 1996.5220947265625\n",
            "Epoch [299/1000] - Loss: 1983.9193115234375\n",
            "Epoch [309/1000] - Loss: 1971.542724609375\n",
            "Epoch [319/1000] - Loss: 1959.388671875\n",
            "Epoch [329/1000] - Loss: 1947.452880859375\n",
            "Epoch [339/1000] - Loss: 1935.7296142578125\n",
            "Epoch [349/1000] - Loss: 1924.2154541015625\n",
            "Epoch [359/1000] - Loss: 1912.9061279296875\n",
            "Epoch [369/1000] - Loss: 1901.7974853515625\n",
            "Epoch [379/1000] - Loss: 1890.8853759765625\n",
            "Epoch [389/1000] - Loss: 1880.16552734375\n",
            "Epoch [399/1000] - Loss: 1869.6346435546875\n",
            "Epoch [409/1000] - Loss: 1859.2882080078125\n",
            "Epoch [419/1000] - Loss: 1849.1236572265625\n",
            "Epoch [429/1000] - Loss: 1839.136474609375\n",
            "Epoch [439/1000] - Loss: 1829.3228759765625\n",
            "Epoch [449/1000] - Loss: 1819.6795654296875\n",
            "Epoch [459/1000] - Loss: 1810.2030029296875\n",
            "Epoch [469/1000] - Loss: 1800.8905029296875\n",
            "Epoch [479/1000] - Loss: 1791.7386474609375\n",
            "Epoch [489/1000] - Loss: 1782.7432861328125\n",
            "Epoch [499/1000] - Loss: 1773.9027099609375\n",
            "Epoch [509/1000] - Loss: 1765.2119140625\n",
            "Epoch [519/1000] - Loss: 1756.6697998046875\n",
            "Epoch [529/1000] - Loss: 1748.272216796875\n",
            "Epoch [539/1000] - Loss: 1740.01708984375\n",
            "Epoch [549/1000] - Loss: 1731.9007568359375\n",
            "Epoch [559/1000] - Loss: 1723.920654296875\n",
            "Epoch [569/1000] - Loss: 1716.0745849609375\n",
            "Epoch [579/1000] - Loss: 1708.359375\n",
            "Epoch [589/1000] - Loss: 1700.7724609375\n",
            "Epoch [599/1000] - Loss: 1693.31201171875\n",
            "Epoch [609/1000] - Loss: 1685.97412109375\n",
            "Epoch [619/1000] - Loss: 1678.75732421875\n",
            "Epoch [629/1000] - Loss: 1671.6595458984375\n",
            "Epoch [639/1000] - Loss: 1664.677734375\n",
            "Epoch [649/1000] - Loss: 1657.8095703125\n",
            "Epoch [659/1000] - Loss: 1651.053466796875\n",
            "Epoch [669/1000] - Loss: 1644.4058837890625\n",
            "Epoch [679/1000] - Loss: 1637.8662109375\n",
            "Epoch [689/1000] - Loss: 1631.4315185546875\n",
            "Epoch [699/1000] - Loss: 1625.100341796875\n",
            "Epoch [709/1000] - Loss: 1618.8695068359375\n",
            "Epoch [719/1000] - Loss: 1612.73828125\n",
            "Epoch [729/1000] - Loss: 1606.7042236328125\n",
            "Epoch [739/1000] - Loss: 1600.76513671875\n",
            "Epoch [749/1000] - Loss: 1594.9193115234375\n",
            "Epoch [759/1000] - Loss: 1589.1654052734375\n",
            "Epoch [769/1000] - Loss: 1583.5010986328125\n",
            "Epoch [779/1000] - Loss: 1577.924560546875\n",
            "Epoch [789/1000] - Loss: 1572.4345703125\n",
            "Epoch [799/1000] - Loss: 1567.029296875\n",
            "Epoch [809/1000] - Loss: 1561.70654296875\n",
            "Epoch [819/1000] - Loss: 1556.4654541015625\n",
            "Epoch [829/1000] - Loss: 1551.3037109375\n",
            "Epoch [839/1000] - Loss: 1546.2205810546875\n",
            "Epoch [849/1000] - Loss: 1541.2144775390625\n",
            "Epoch [859/1000] - Loss: 1536.283447265625\n",
            "Epoch [869/1000] - Loss: 1531.426025390625\n",
            "Epoch [879/1000] - Loss: 1526.640625\n",
            "Epoch [889/1000] - Loss: 1521.9271240234375\n",
            "Epoch [899/1000] - Loss: 1517.2822265625\n",
            "Epoch [909/1000] - Loss: 1512.7064208984375\n",
            "Epoch [919/1000] - Loss: 1508.197265625\n",
            "Epoch [929/1000] - Loss: 1503.75390625\n",
            "Epoch [939/1000] - Loss: 1499.375\n",
            "Epoch [949/1000] - Loss: 1495.059326171875\n",
            "Epoch [959/1000] - Loss: 1490.805419921875\n",
            "Epoch [969/1000] - Loss: 1486.612548828125\n",
            "Epoch [979/1000] - Loss: 1482.4796142578125\n",
            "Epoch [989/1000] - Loss: 1478.4049072265625\n",
            "Epoch [999/1000] - Loss: 1474.3876953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1f5IyLf_3hD",
        "colab_type": "code",
        "outputId": "6e70850a-2b7f-4d59-d2ff-7f8330787660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "# Plotting the loss\n",
        "\n",
        "figure = go.Figure()\n",
        "figure.add_trace(\n",
        "    go.Scatter(\n",
        "        x=np.arange(Loss.size),\n",
        "        y=Loss,\n",
        "        mode='lines+markers',\n",
        "        name='Training loss',\n",
        "        line=go.scatter.Line(color='blue')\n",
        "    )\n",
        ")\n",
        "figure.update_layout(\n",
        "    title='Training loss',\n",
        "    title_font_size=22\n",
        ")\n",
        "figure.show()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e10cc801-1e04-483b-b3ad-0e8d67a9cc91\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e10cc801-1e04-483b-b3ad-0e8d67a9cc91\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e10cc801-1e04-483b-b3ad-0e8d67a9cc91',\n",
              "                        [{\"line\": {\"color\": \"blue\"}, \"mode\": \"lines+markers\", \"name\": \"Training loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999], \"y\": [13274.54296875, 11221.8115234375, 9558.7265625, 8211.2470703125, 7119.404296875, 6234.62109375, 5517.5556640625, 4936.33837890625, 4465.1552734375, 4083.1005859375, 3773.239501953125, 3521.85205078125, 3317.830322265625, 3152.173828125, 3017.593505859375, 2908.18505859375, 2819.16552734375, 2746.662353515625, 2687.53759765625, 2639.25, 2599.740478515625, 2567.342041015625, 2540.705078125, 2518.73486328125, 2500.544921875, 2485.41845703125, 2472.773681640625, 2462.140380859375, 2453.13623046875, 2445.451904296875, 2438.839111328125, 2433.094970703125, 2428.05322265625, 2423.582275390625, 2419.5751953125, 2415.943115234375, 2412.61572265625, 2409.53759765625, 2406.659912109375, 2403.946533203125, 2401.36669921875, 2398.895263671875, 2396.513427734375, 2394.20361328125, 2391.95361328125, 2389.7529296875, 2387.593017578125, 2385.4658203125, 2383.366943359375, 2381.29052734375, 2379.2353515625, 2377.195556640625, 2375.17041015625, 2373.15673828125, 2371.153564453125, 2369.16015625, 2367.17529296875, 2365.196533203125, 2363.2255859375, 2361.2607421875, 2359.30078125, 2357.3466796875, 2355.397216796875, 2353.452392578125, 2351.512939453125, 2349.576904296875, 2347.645263671875, 2345.7177734375, 2343.794921875, 2341.875244140625, 2339.959716796875, 2338.048095703125, 2336.139892578125, 2334.2353515625, 2332.3349609375, 2330.43798828125, 2328.544921875, 2326.65576171875, 2324.77001953125, 2322.887939453125, 2321.00927734375, 2319.134033203125, 2317.262451171875, 2315.39501953125, 2313.530029296875, 2311.6689453125, 2309.8125, 2307.95849609375, 2306.10791015625, 2304.260986328125, 2302.41845703125, 2300.57861328125, 2298.742431640625, 2296.90966796875, 2295.080078125, 2293.254638671875, 2291.43212890625, 2289.61328125, 2287.7978515625, 2285.98583984375, 2284.177490234375, 2282.372314453125, 2280.57080078125, 2278.77197265625, 2276.97705078125, 2275.185546875, 2273.39697265625, 2271.6123046875, 2269.831298828125, 2268.05322265625, 2266.27783203125, 2264.507080078125, 2262.73876953125, 2260.97412109375, 2259.212890625, 2257.45458984375, 2255.700439453125, 2253.948486328125, 2252.200439453125, 2250.455810546875, 2248.71435546875, 2246.9765625, 2245.24072265625, 2243.50927734375, 2241.78125, 2240.055908203125, 2238.333984375, 2236.615478515625, 2234.899658203125, 2233.188232421875, 2231.478515625, 2229.773193359375, 2228.070068359375, 2226.370849609375, 2224.674072265625, 2222.981689453125, 2221.2919921875, 2219.60546875, 2217.92138671875, 2216.241943359375, 2214.56396484375, 2212.890380859375, 2211.21923828125, 2209.551513671875, 2207.886962890625, 2206.225341796875, 2204.5673828125, 2202.912109375, 2201.26025390625, 2199.61083984375, 2197.96484375, 2196.32177734375, 2194.68212890625, 2193.044921875, 2191.41162109375, 2189.78076171875, 2188.153564453125, 2186.528564453125, 2184.907470703125, 2183.2890625, 2181.673095703125, 2180.06103515625, 2178.45166015625, 2176.845458984375, 2175.2421875, 2173.64111328125, 2172.04443359375, 2170.449951171875, 2168.859375, 2167.27001953125, 2165.684814453125, 2164.1025390625, 2162.52197265625, 2160.94580078125, 2159.371826171875, 2157.80126953125, 2156.233642578125, 2154.668701171875, 2153.10693359375, 2151.54736328125, 2149.9912109375, 2148.438232421875, 2146.8876953125, 2145.33984375, 2143.79541015625, 2142.25390625, 2140.71484375, 2139.179443359375, 2137.6455078125, 2136.115478515625, 2134.588134765625, 2133.063720703125, 2131.541748046875, 2130.022705078125, 2128.506591796875, 2126.993408203125, 2125.482421875, 2123.97509765625, 2122.47021484375, 2120.968017578125, 2119.46923828125, 2117.97265625, 2116.478515625, 2114.987548828125, 2113.499267578125, 2112.013671875, 2110.531005859375, 2109.05078125, 2107.573486328125, 2106.098876953125, 2104.627197265625, 2103.15869140625, 2101.69189453125, 2100.228515625, 2098.76708984375, 2097.30908203125, 2095.854248046875, 2094.40087890625, 2092.950927734375, 2091.503173828125, 2090.05859375, 2088.616455078125, 2087.176513671875, 2085.74072265625, 2084.305908203125, 2082.875, 2081.4453125, 2080.01953125, 2078.595947265625, 2077.17431640625, 2075.7568359375, 2074.3408203125, 2072.927001953125, 2071.517578125, 2070.109375, 2068.70361328125, 2067.30126953125, 2065.90087890625, 2064.503173828125, 2063.108642578125, 2061.716064453125, 2060.326171875, 2058.93896484375, 2057.553955078125, 2056.171875, 2054.792236328125, 2053.414794921875, 2052.040283203125, 2050.668701171875, 2049.29833984375, 2047.9315185546875, 2046.567138671875, 2045.204833984375, 2043.845458984375, 2042.48779296875, 2041.133544921875, 2039.781494140625, 2038.4320068359375, 2037.0845947265625, 2035.7401123046875, 2034.39794921875, 2033.05810546875, 2031.7205810546875, 2030.3853759765625, 2029.053466796875, 2027.7230224609375, 2026.396240234375, 2025.071044921875, 2023.7484130859375, 2022.427734375, 2021.1097412109375, 2019.794677734375, 2018.4814453125, 2017.170654296875, 2015.8624267578125, 2014.5565185546875, 2013.2532958984375, 2011.9521484375, 2010.6533203125, 2009.3568115234375, 2008.0628662109375, 2006.771240234375, 2005.48193359375, 2004.1953125, 2002.91015625, 2001.6280517578125, 2000.3480224609375, 1999.0706787109375, 1997.795166015625, 1996.5220947265625, 1995.251708984375, 1993.9830322265625, 1992.717529296875, 1991.453369140625, 1990.192138671875, 1988.9329833984375, 1987.676025390625, 1986.4217529296875, 1985.1689453125, 1983.9193115234375, 1982.6712646484375, 1981.425537109375, 1980.1826171875, 1978.941650390625, 1977.702880859375, 1976.466552734375, 1975.2322998046875, 1974.0003662109375, 1972.7706298828125, 1971.542724609375, 1970.3175048828125, 1969.0947265625, 1967.8736572265625, 1966.6549072265625, 1965.4384765625, 1964.2236328125, 1963.011962890625, 1961.80224609375, 1960.5948486328125, 1959.388671875, 1958.18505859375, 1956.9842529296875, 1955.785400390625, 1954.588623046875, 1953.393310546875, 1952.201171875, 1951.010498046875, 1949.8228759765625, 1948.63623046875, 1947.452880859375, 1946.2706298828125, 1945.0908203125, 1943.9134521484375, 1942.7381591796875, 1941.5648193359375, 1940.3936767578125, 1939.224365234375, 1938.0570068359375, 1936.8924560546875, 1935.7296142578125, 1934.5689697265625, 1933.410400390625, 1932.2532958984375, 1931.099365234375, 1929.94677734375, 1928.7960205078125, 1927.648193359375, 1926.501708984375, 1925.3577880859375, 1924.2154541015625, 1923.0755615234375, 1921.9378662109375, 1920.8011474609375, 1919.667724609375, 1918.5355224609375, 1917.406005859375, 1916.2777099609375, 1915.1513671875, 1914.028076171875, 1912.9061279296875, 1911.78662109375, 1910.6685791015625, 1909.5526123046875, 1908.43896484375, 1907.3272705078125, 1906.2174072265625, 1905.109375, 1904.00341796875, 1902.8995361328125, 1901.7974853515625, 1900.697509765625, 1899.5994873046875, 1898.50341796875, 1897.4093017578125, 1896.316650390625, 1895.2269287109375, 1894.138427734375, 1893.05224609375, 1891.9677734375, 1890.8853759765625, 1889.8045654296875, 1888.726318359375, 1887.649169921875, 1886.5745849609375, 1885.501708984375, 1884.4307861328125, 1883.3616943359375, 1882.2943115234375, 1881.229248046875, 1880.16552734375, 1879.1044921875, 1878.04443359375, 1876.987060546875, 1875.9307861328125, 1874.876708984375, 1873.824951171875, 1872.7747802734375, 1871.72607421875, 1870.679443359375, 1869.6346435546875, 1868.591552734375, 1867.5506591796875, 1866.511962890625, 1865.474365234375, 1864.4385986328125, 1863.405029296875, 1862.3736572265625, 1861.3431396484375, 1860.3148193359375, 1859.2882080078125, 1858.2640380859375, 1857.2413330078125, 1856.220458984375, 1855.201416015625, 1854.1834716796875, 1853.168212890625, 1852.154296875, 1851.1424560546875, 1850.1322021484375, 1849.1236572265625, 1848.1171875, 1847.112060546875, 1846.1087646484375, 1845.1077880859375, 1844.1077880859375, 1843.1104736328125, 1842.1142578125, 1841.119873046875, 1840.1273193359375, 1839.136474609375, 1838.147216796875, 1837.15966796875, 1836.173828125, 1835.190185546875, 1834.2080078125, 1833.2276611328125, 1832.2484130859375, 1831.2718505859375, 1830.2960205078125, 1829.3228759765625, 1828.3507080078125, 1827.380615234375, 1826.412109375, 1825.4453125, 1824.4798583984375, 1823.516357421875, 1822.5546875, 1821.594482421875, 1820.63623046875, 1819.6795654296875, 1818.7244873046875, 1817.7713623046875, 1816.8194580078125, 1815.8690185546875, 1814.920654296875, 1813.9739990234375, 1813.029052734375, 1812.08544921875, 1811.143310546875, 1810.2030029296875, 1809.2650146484375, 1808.3280029296875, 1807.3924560546875, 1806.458740234375, 1805.5267333984375, 1804.596435546875, 1803.6678466796875, 1802.7401123046875, 1801.814697265625, 1800.8905029296875, 1799.9686279296875, 1799.0478515625, 1798.12841796875, 1797.210693359375, 1796.294677734375, 1795.38037109375, 1794.4674072265625, 1793.55615234375, 1792.6468505859375, 1791.7386474609375, 1790.8319091796875, 1789.9271240234375, 1789.0240478515625, 1788.1221923828125, 1787.221435546875, 1786.3226318359375, 1785.4256591796875, 1784.530029296875, 1783.6358642578125, 1782.7432861328125, 1781.852294921875, 1780.9632568359375, 1780.0753173828125, 1779.1885986328125, 1778.3040771484375, 1777.420654296875, 1776.53857421875, 1775.658447265625, 1774.780029296875, 1773.9027099609375, 1773.0260009765625, 1772.15234375, 1771.279541015625, 1770.4085693359375, 1769.538818359375, 1768.669921875, 1767.8033447265625, 1766.938232421875, 1766.0745849609375, 1765.2119140625, 1764.3514404296875, 1763.4920654296875, 1762.6339111328125, 1761.7777099609375, 1760.9228515625, 1760.0693359375, 1759.2174072265625, 1758.36669921875, 1757.5174560546875, 1756.6697998046875, 1755.8236083984375, 1754.9791259765625, 1754.1353759765625, 1753.2939453125, 1752.453369140625, 1751.6138916015625, 1750.7764892578125, 1749.94091796875, 1749.1053466796875, 1748.272216796875, 1747.440185546875, 1746.6099853515625, 1745.781005859375, 1744.953369140625, 1744.126953125, 1743.30224609375, 1742.4791259765625, 1741.6568603515625, 1740.836181640625, 1740.01708984375, 1739.19873046875, 1738.3826904296875, 1737.5675048828125, 1736.754150390625, 1735.94140625, 1735.130859375, 1734.321044921875, 1733.5128173828125, 1732.705810546875, 1731.9007568359375, 1731.0966796875, 1730.2940673828125, 1729.492431640625, 1728.6923828125, 1727.8936767578125, 1727.096435546875, 1726.300537109375, 1725.505859375, 1724.713134765625, 1723.920654296875, 1723.13037109375, 1722.340576171875, 1721.552734375, 1720.7666015625, 1719.9810791015625, 1719.197021484375, 1718.4144287109375, 1717.6334228515625, 1716.853271484375, 1716.0745849609375, 1715.297119140625, 1714.521240234375, 1713.7467041015625, 1712.972900390625, 1712.200927734375, 1711.4296875, 1710.660400390625, 1709.8924560546875, 1709.125244140625, 1708.359375, 1707.5947265625, 1706.8323974609375, 1706.06982421875, 1705.309326171875, 1704.5499267578125, 1703.7923583984375, 1703.0355224609375, 1702.280029296875, 1701.5260009765625, 1700.7724609375, 1700.0208740234375, 1699.270263671875, 1698.521240234375, 1697.7734375, 1697.0267333984375, 1696.28125, 1695.5369873046875, 1694.7940673828125, 1694.052490234375, 1693.31201171875, 1692.572509765625, 1691.8345947265625, 1691.09765625, 1690.3619384765625, 1689.6280517578125, 1688.8944091796875, 1688.162841796875, 1687.4320068359375, 1686.70263671875, 1685.97412109375, 1685.2474365234375, 1684.521240234375, 1683.796875, 1683.0732421875, 1682.3509521484375, 1681.6300048828125, 1680.909912109375, 1680.1917724609375, 1679.4737548828125, 1678.75732421875, 1678.04248046875, 1677.328369140625, 1676.615966796875, 1675.9041748046875, 1675.1937255859375, 1674.4847412109375, 1673.7767333984375, 1673.0694580078125, 1672.3638916015625, 1671.6595458984375, 1670.9561767578125, 1670.2537841796875, 1669.552734375, 1668.85302734375, 1668.1541748046875, 1667.4564208984375, 1666.760009765625, 1666.064453125, 1665.3704833984375, 1664.677734375, 1663.9857177734375, 1663.2947998046875, 1662.6051025390625, 1661.9169921875, 1661.2296142578125, 1660.543212890625, 1659.8580322265625, 1659.173828125, 1658.490966796875, 1657.8095703125, 1657.129150390625, 1656.449462890625, 1655.7708740234375, 1655.0936279296875, 1654.4173583984375, 1653.7425537109375, 1653.068603515625, 1652.3955078125, 1651.7237548828125, 1651.053466796875, 1650.383544921875, 1649.715087890625, 1649.0474853515625, 1648.3814697265625, 1647.7158203125, 1647.0521240234375, 1646.388916015625, 1645.726806640625, 1645.0657958984375, 1644.4058837890625, 1643.74755859375, 1643.0894775390625, 1642.43310546875, 1641.777587890625, 1641.123046875, 1640.4693603515625, 1639.8173828125, 1639.166015625, 1638.5155029296875, 1637.8662109375, 1637.21826171875, 1636.571044921875, 1635.92529296875, 1635.2799072265625, 1634.6358642578125, 1633.9931640625, 1633.3511962890625, 1632.71044921875, 1632.0701904296875, 1631.4315185546875, 1630.7940673828125, 1630.156982421875, 1629.521728515625, 1628.886962890625, 1628.2535400390625, 1627.62060546875, 1626.989013671875, 1626.3585205078125, 1625.7291259765625, 1625.100341796875, 1624.4725341796875, 1623.8463134765625, 1623.220703125, 1622.5963134765625, 1621.97265625, 1621.3502197265625, 1620.7281494140625, 1620.1077880859375, 1619.48828125, 1618.8695068359375, 1618.251953125, 1617.6357421875, 1617.0201416015625, 1616.4053955078125, 1615.791748046875, 1615.1793212890625, 1614.5675048828125, 1613.9566650390625, 1613.34716796875, 1612.73828125, 1612.1304931640625, 1611.5238037109375, 1610.91796875, 1610.3131103515625, 1609.709228515625, 1609.1060791015625, 1608.504638671875, 1607.903076171875, 1607.3031005859375, 1606.7042236328125, 1606.1060791015625, 1605.508544921875, 1604.9124755859375, 1604.3170166015625, 1603.722900390625, 1603.129150390625, 1602.5367431640625, 1601.945556640625, 1601.3548583984375, 1600.76513671875, 1600.176513671875, 1599.588623046875, 1599.0013427734375, 1598.4156494140625, 1597.830810546875, 1597.2467041015625, 1596.6632080078125, 1596.0811767578125, 1595.5, 1594.9193115234375, 1594.33984375, 1593.7613525390625, 1593.1839599609375, 1592.6065673828125, 1592.0308837890625, 1591.4561767578125, 1590.8818359375, 1590.3092041015625, 1589.7369384765625, 1589.1654052734375, 1588.5948486328125, 1588.0252685546875, 1587.4566650390625, 1586.888916015625, 1586.3218994140625, 1585.755859375, 1585.19091796875, 1584.6268310546875, 1584.0634765625, 1583.5010986328125, 1582.93896484375, 1582.37841796875, 1581.8189697265625, 1581.2596435546875, 1580.701904296875, 1580.144775390625, 1579.5882568359375, 1579.0325927734375, 1578.4781494140625, 1577.924560546875, 1577.3717041015625, 1576.81982421875, 1576.2686767578125, 1575.71826171875, 1575.168701171875, 1574.6201171875, 1574.0726318359375, 1573.525390625, 1572.9796142578125, 1572.4345703125, 1571.89013671875, 1571.3468017578125, 1570.8040771484375, 1570.2623291015625, 1569.7213134765625, 1569.1812744140625, 1568.6417236328125, 1568.1033935546875, 1567.56591796875, 1567.029296875, 1566.492919921875, 1565.9576416015625, 1565.4237060546875, 1564.8902587890625, 1564.3572998046875, 1563.8255615234375, 1563.2945556640625, 1562.7646484375, 1562.2349853515625, 1561.70654296875, 1561.1788330078125, 1560.6517333984375, 1560.1256103515625, 1559.600341796875, 1559.0758056640625, 1558.5518798828125, 1558.029296875, 1557.5072021484375, 1556.9857177734375, 1556.4654541015625, 1555.945556640625, 1555.4267578125, 1554.9085693359375, 1554.3916015625, 1553.87451171875, 1553.3590087890625, 1552.8441162109375, 1552.3299560546875, 1551.816650390625, 1551.3037109375, 1550.7921142578125, 1550.2811279296875, 1549.7708740234375, 1549.26123046875, 1548.752685546875, 1548.2447509765625, 1547.73779296875, 1547.231201171875, 1546.7255859375, 1546.2205810546875, 1545.717041015625, 1545.2135009765625, 1544.7109375, 1544.209228515625, 1543.707763671875, 1543.2080078125, 1542.70849609375, 1542.2095947265625, 1541.7119140625, 1541.2144775390625, 1540.7178955078125, 1540.2222900390625, 1539.727294921875, 1539.2330322265625, 1538.7396240234375, 1538.246826171875, 1537.7548828125, 1537.2635498046875, 1536.7733154296875, 1536.283447265625, 1535.79443359375, 1535.3057861328125, 1534.818603515625, 1534.33154296875, 1533.8455810546875, 1533.360107421875, 1532.8756103515625, 1532.3919677734375, 1531.908203125, 1531.426025390625, 1530.944091796875, 1530.462890625, 1529.9827880859375, 1529.5032958984375, 1529.0247802734375, 1528.54638671875, 1528.068603515625, 1527.592041015625, 1527.115966796875, 1526.640625, 1526.1663818359375, 1525.6925048828125, 1525.2193603515625, 1524.746826171875, 1524.2750244140625, 1523.8040771484375, 1523.3336181640625, 1522.8642578125, 1522.3951416015625, 1521.9271240234375, 1521.4593505859375, 1520.9925537109375, 1520.5263671875, 1520.060791015625, 1519.595703125, 1519.1322021484375, 1518.668701171875, 1518.205810546875, 1517.7437744140625, 1517.2822265625, 1516.8218994140625, 1516.3616943359375, 1515.9022216796875, 1515.4439697265625, 1514.9862060546875, 1514.5286865234375, 1514.0721435546875, 1513.6163330078125, 1513.160888671875, 1512.7064208984375, 1512.2523193359375, 1511.7994384765625, 1511.34716796875, 1510.894775390625, 1510.443603515625, 1509.9931640625, 1509.543212890625, 1509.09375, 1508.6451416015625, 1508.197265625, 1507.7498779296875, 1507.303466796875, 1506.8572998046875, 1506.412109375, 1505.967529296875, 1505.5233154296875, 1505.080322265625, 1504.637451171875, 1504.195556640625, 1503.75390625, 1503.3134765625, 1502.8726806640625, 1502.43359375, 1501.9945068359375, 1501.55615234375, 1501.1187744140625, 1500.681884765625, 1500.24560546875, 1499.8101806640625, 1499.375, 1498.9407958984375, 1498.5069580078125, 1498.0736083984375, 1497.6409912109375, 1497.2093505859375, 1496.778076171875, 1496.3472900390625, 1495.9173583984375, 1495.48779296875, 1495.059326171875, 1494.630859375, 1494.203369140625, 1493.7764892578125, 1493.350341796875, 1492.9244384765625, 1492.4996337890625, 1492.074951171875, 1491.6514892578125, 1491.2279052734375, 1490.805419921875, 1490.3831787109375, 1489.9622802734375, 1489.541259765625, 1489.120849609375, 1488.7012939453125, 1488.2825927734375, 1487.8641357421875, 1487.4464111328125, 1487.0291748046875, 1486.612548828125, 1486.196533203125, 1485.7811279296875, 1485.3665771484375, 1484.9525146484375, 1484.5384521484375, 1484.1258544921875, 1483.71337890625, 1483.3013916015625, 1482.890380859375, 1482.4796142578125, 1482.0694580078125, 1481.659912109375, 1481.251220703125, 1480.8427734375, 1480.4349365234375, 1480.027587890625, 1479.6214599609375, 1479.21533203125, 1478.8099365234375, 1478.4049072265625, 1478.000732421875, 1477.5970458984375, 1477.1934814453125, 1476.7911376953125, 1476.3892822265625, 1475.9876708984375, 1475.5867919921875, 1475.1865234375, 1474.7869873046875, 1474.3876953125]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"font\": {\"size\": 22}, \"text\": \"Training loss\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e10cc801-1e04-483b-b3ad-0e8d67a9cc91');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARVlaDMbwlgH",
        "colab_type": "text"
      },
      "source": [
        "# Using built in LR model\n",
        "\n",
        "Pytorch provides built in models in the class _torch.nn_. In order to use it we must define:\n",
        "\n",
        "* Dataset object within a DataLoader object (optional but convenient)\n",
        "* Loss function\n",
        "* Optimizer\n",
        "* Training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25KcQ-jhkN66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgkX-uoEkWd5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "93101b23-296f-4aa1-ba1e-dffd1dc95d4b"
      },
      "source": [
        "# TensorDataset\n",
        "# Dataset wrapping tensors as a tuple\n",
        "# Each sample will be retrieved by indexing tensors along the first dimension\n",
        "# Source: https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n",
        "\n",
        "training_dataset = TensorDataset(X_training_tensor, y_training_tensor)\n",
        "print(f'First sample:\\n{training_dataset[0]}\\n')\n",
        "print(f'First sample (Input tensor):\\n{training_dataset[0][0]}\\n')\n",
        "print(f'First sample (Output tensor):\\n{training_dataset[0][1]}\\n')\n",
        "print(f'Training samples: {len(training_dataset)}')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First sample:\n",
            "(tensor([-0.0237, -0.0446,  0.0455,  0.0907, -0.0181, -0.0354,  0.0707, -0.0395,\n",
            "        -0.0345, -0.0094]), tensor(175.))\n",
            "\n",
            "First sample (Input tensor):\n",
            "tensor([-0.0237, -0.0446,  0.0455,  0.0907, -0.0181, -0.0354,  0.0707, -0.0395,\n",
            "        -0.0345, -0.0094])\n",
            "\n",
            "First sample (Output tensor):\n",
            "175.0\n",
            "\n",
            "Training samples: 88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGzRbb4NAz5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ef719336-d77d-40b8-f82f-467244292bc0"
      },
      "source": [
        "# Define the DataLoader\n",
        "# Return an iterable object\n",
        "batch_size = 5\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
        "for xi, yi in training_dataloader:\n",
        "    print(xi)\n",
        "    print(yi)\n",
        "    break\n",
        "\n",
        "# As we can see, in one iteration it fetches 5 data samples"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0600,  0.0507, -0.0472, -0.0229, -0.0717, -0.0577, -0.0066, -0.0395,\n",
            "         -0.0629, -0.0549],\n",
            "        [ 0.0272, -0.0446, -0.0073, -0.0504,  0.0755,  0.0566,  0.0339, -0.0026,\n",
            "          0.0434,  0.0155],\n",
            "        [ 0.0054, -0.0446,  0.0585, -0.0435, -0.0731, -0.0724,  0.0192, -0.0764,\n",
            "         -0.0514, -0.0259],\n",
            "        [ 0.0054,  0.0507, -0.0084,  0.0219,  0.0548,  0.0732, -0.0250,  0.0343,\n",
            "          0.0126,  0.0942],\n",
            "        [ 0.0018,  0.0507, -0.0062, -0.0194, -0.0098,  0.0049, -0.0397,  0.0343,\n",
            "          0.0148,  0.0983]])\n",
            "tensor([ 72.,  95., 136., 100., 262.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2-8XXQNI7mq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6d4fd79e-5b68-416f-f078-ffbd475a33de"
      },
      "source": [
        "# Creating the model\n",
        "model = nn.Linear(len(features), 1)\n",
        "\n",
        "print(model.weight, end='\\n\\n')\n",
        "print(model.bias)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.2748,  0.3052,  0.3019, -0.2177,  0.0782,  0.2614,  0.0612,  0.1360,\n",
            "          0.2355,  0.1342]], requires_grad=True)\n",
            "\n",
            "Parameter containing:\n",
            "tensor([-0.0572], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4VX0DQUJeCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f95c91aa-5182-43f3-b5b4-7b9223a929fa"
      },
      "source": [
        "# Parameters\n",
        "list(model.parameters())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.2748,  0.3052,  0.3019, -0.2177,  0.0782,  0.2614,  0.0612,  0.1360,\n",
              "           0.2355,  0.1342]], requires_grad=True), Parameter containing:\n",
              " tensor([-0.0572], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MybRxk4sJjtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8cbb835c-6dc5-43e8-b5a7-650da0ca142e"
      },
      "source": [
        "# Loss function\n",
        "# We will use the MSE as we did in the manually created model\n",
        "loss_fn = F.mse_loss\n",
        "\n",
        "# Optimizer \n",
        "# Stochastic Gradient Descent\n",
        "# Using mini batches in each iteration instead of one by one\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-05)\n",
        "opt"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    lr: 1e-05\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slu9ap2fLjPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the training loop\n",
        "def fit(epochs, model, loss_fn, optimizer):\n",
        "    Loss = []\n",
        "    for e in range(epochs):\n",
        "        for xi, yi in training_dataloader:\n",
        "            y_pred = model(xi)\n",
        "            loss = loss_fn(y_pred, yi)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        if (e+1)%10 == 0:\n",
        "            print(f'Epoch[{e}/{epochs}], Loss = {loss}')\n",
        "        Loss.append(loss.item())\n",
        "    return np.array(Loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQj-na7rMm-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "427788df-7d3a-4003-fca5-f86b0ac7afba"
      },
      "source": [
        "model_Loss = fit(50, model=model, loss_fn=loss_fn, optimizer=opt)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning:\n",
            "\n",
            "Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning:\n",
            "\n",
            "Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch[9/50], Loss = 3216.864990234375\n",
            "Epoch[19/50], Loss = 3730.44482421875\n",
            "Epoch[29/50], Loss = 2586.797119140625\n",
            "Epoch[39/50], Loss = 3359.46728515625\n",
            "Epoch[49/50], Loss = 1582.2579345703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHXmeyMGOEf5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "72722b76-b29b-4a99-c28e-f942576311ff"
      },
      "source": [
        "figure = go.Figure()\n",
        "figure.add_trace(\n",
        "    go.Scatter(\n",
        "        x=np.arange(model_Loss.size),\n",
        "        y=model_Loss,\n",
        "        mode='lines+markers',\n",
        "        name='Training loss',\n",
        "        line=go.scatter.Line(color='blue')\n",
        "    )\n",
        ")\n",
        "figure.update_layout(\n",
        "    title='Training loss',\n",
        "    title_font_size=22\n",
        ")\n",
        "figure.show()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"bf502b12-dcf5-4b66-9e64-392c3ad6616b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"bf502b12-dcf5-4b66-9e64-392c3ad6616b\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'bf502b12-dcf5-4b66-9e64-392c3ad6616b',\n",
              "                        [{\"line\": {\"color\": \"blue\"}, \"mode\": \"lines+markers\", \"name\": \"Training loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], \"y\": [4730.13671875, 6420.287109375, 9314.93359375, 3409.89404296875, 8467.7470703125, 7280.40869140625, 4202.611328125, 3887.032958984375, 7949.076171875, 3216.864990234375, 3553.864501953125, 3531.7060546875, 1429.0269775390625, 7231.17041015625, 3559.70849609375, 3392.383544921875, 477.8759765625, 3616.6982421875, 5672.255859375, 3730.44482421875, 3515.22509765625, 8367.4951171875, 3043.1611328125, 7608.66943359375, 1891.77099609375, 3195.5771484375, 1057.55224609375, 5701.46337890625, 5915.802734375, 2586.797119140625, 5715.34521484375, 2213.80908203125, 4057.5625, 3456.427978515625, 3490.08642578125, 2668.51953125, 6476.94873046875, 8967.322265625, 4304.50244140625, 3359.46728515625, 10329.80078125, 2378.73681640625, 2392.180908203125, 3792.435791015625, 3785.85888671875, 11118.55078125, 8261.49609375, 676.3988647460938, 7330.3056640625, 1582.2579345703125]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"font\": {\"size\": 22}, \"text\": \"Training loss\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bf502b12-dcf5-4b66-9e64-392c3ad6616b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}