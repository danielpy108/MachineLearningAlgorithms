{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0ZyG0EKzlrC9iCQTAKuMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielpy108/MachineLearningAlgorithms/blob/master/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsNKEpd-9hX_",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression \n",
        "\n",
        "Linear regression is a parametric supervised learning model that fits the line which sum of residual errors is the least, to the training data that is suplied as input.\n",
        "\n",
        "This ML model can be used for:\n",
        "+ Classification: only linearly separable data\n",
        "+ Regression: every kind of data (it's not the best always)\n",
        "\n",
        "## Regression\n",
        "\n",
        "For regression problems, the input vectors are real valued and the output ones are also real values, this means it's a mapping from $R^n \\rightarrow R^n$.\n",
        "\n",
        "We have a traning dataset $D = \\{(x_1, y_1), ..., (x_N, y_N)\\}$ of N samples, for each input vector $x_i$ its dimenssion or number of features is $d$: $x_i = (x_{i1}, ..., x_{id})$.\n",
        "\n",
        "The linear regression model has parameters that can be modified or adjusted in order to fit best the line, plane or hyperplane to the data points. This parameters are most commonly known as **__weights__**. \n",
        "\n",
        "In the model, we will have as many parameters as the number of features, so if we want to predict the values of the input given the _weights_ and the _feature vector_:\n",
        "\n",
        "$$ y = \\sum_i{\\alpha_i x_i} + b$$\n",
        "\n",
        "For the following examples, let's suppose we want to know what the value of y is given that our feature vector lies in a 1D space: $x_i \\in R^1$.\n",
        "\n",
        "$$y = \\alpha_1 x_1 + b$$\n",
        "\n",
        "This equation reminds you of something, right? Basically, $\\alpha$ is the slope of the line and $b$ is the intersection with the y-axis.\n",
        "\n",
        "It's worth to mention that the __initial parameters__ are set to zero.\n",
        "\n",
        "## Learning from inputs\n",
        "\n",
        "To make the linear regression model learn from the dataset of N samples $D$, we need:\n",
        "\n",
        "1. A Loss function\n",
        "2. An error function\n",
        "3. Miminize the error with respect of the parameters $\\alpha$ and $b$\n",
        "4. Update the parameters \n",
        "\n",
        "This procedure must be run inside a loop of _e_ iterations (every iteration is also called an **_epoch_**). This way, the parameters will be updated and the hyperplane with the least sum of  residual errors will be fitted.\n",
        "\n",
        "### A Loss and an error Function \n",
        "The Loss function is the sum of the error function's value for each sample $(x_i, y_i)$:\n",
        "\n",
        "$$ L = \\sum_i{l_i} $$\n",
        "\n",
        "$l_i$ is the error function for a single sample. Altough, there are many error functions, in Linear Regression is commonly used the MSE (Mean Squared Error).\n",
        "\n",
        "$$ L = \\sum_i{\\frac{1}{2N}(y_i - \\hat{y}_i)^2} $$\n",
        "$$ L = \\frac{1}{2N}\\sum_i{(y_i - \\hat{y}_i)^2} $$\n",
        "$$ L = \\frac{1}{2N}\\sum_i{(y_i - (\\sum_j{\\alpha_j x_j+b}))^2} $$\n",
        "\n",
        "The Loss function is a cumulative sum of the singular errors and its value changes for different values of $\\alpha_i$.\n",
        "\n",
        "### Minizing the Loss function\n",
        "Since we want to minize the Loss with respect to the parameters model to find the best hyperplane, let's take the two partial derivatives we need:\n",
        "\n",
        "#### With respect of alpha\n",
        "$$ \\Delta_{\\alpha_i}{L} =  \\frac{\\partial{L}}{\\partial{\\alpha}}  $$\n",
        "$$ \\Delta_{\\alpha_i}{L} =  \\frac{1}{2N}\\sum_i{\\frac{\\partial{}}{\\partial{\\alpha_i}}}(y_i - (\\sum_j{\\alpha_j x_j+b}))^2 $$\n",
        "\n",
        "In the second sum term its value is zero everything except when $i = j$, that's the associated \"coeficient\" $x_j$ of the $\\alpha_i$.\n",
        "\n",
        "$$ \\Delta_{\\alpha_i}{L} = \\frac{1}{N}\\sum_i (y_i - (\\sum_j{\\alpha_j x_j+b})) x_j $$\n",
        "\n",
        "#### With respect of the bias \n",
        "$$ \\Delta_{b}{L} =  \\frac{\\partial{L}}{\\partial{b}}  $$\n",
        "$$ \\Delta_{b}{L} =  \\frac{1}{2N}\\sum_i{\\frac{\\partial{}}{\\partial{\\alpha_i}}}(y_i - (\\sum_j{\\alpha_j x_j+b}))^2 $$\n",
        "\n",
        "$$ \\Delta_{b}{L} = \\frac{1}{N}\\sum_i (y_i - (\\sum_j{\\alpha_j x_j+b}))$$\n",
        "\n",
        "### Updating the parameters\n",
        "\n",
        "Once we find the gradient of the Loss function with respect to the $\\alpha's$ and the bias term $b$, we are ready to update the weights:\n",
        "\n",
        "$$ \\alpha_i = \\alpha_i - \\mu\\Delta_{\\alpha_i}{L} $$\n",
        "$$ \\alpha_i = \\alpha_i - \\frac{\\mu}{N}\\sum_i (y_i - (\\sum_j{\\alpha_j x_j+b})) $$\n",
        "\n",
        "$$ b = b - \\mu\\Delta_{b}{L} $$\n",
        "$$ b = \\alpha - \\frac{\\mu}{N}\\sum_i (y_i - (\\sum_j{\\alpha_j x_j+b})) $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdKDTM-V9c6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSNd4ZPQMm5W",
        "colab_type": "code",
        "outputId": "1b0f5167-3854-45a9-b189-74f1bb30503d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Let's generate a random Dataset for Regression\n",
        "# In sklearn is as simple as:\n",
        "#   X, y = datasets.make_regression(n_samples=100, n_features=3, noise=20, random_state=1)\n",
        "torch.random.manual_seed(9)\n",
        "a, b = 5, -5                              # Random distribution between -5 and 5\n",
        "N = 100                                   # Number of samples \n",
        "d = 3                                     # Number of features (input dimenssion)\n",
        "D = (a-b)*torch.rand(size=(N, d+1), dtype=torch.float32) + b   # Dataset (the 4th column is the target variable y)\n",
        "X = D[:,:3]                               # Input features\n",
        "Y = D[:,3]                                # Target features\n",
        "\n",
        "# Let's generate random parameters (alpha's and bias)\n",
        "W = torch.rand(size=(d,), dtype=torch.float32, requires_grad=True)\n",
        "b = torch.rand(size=(N,), dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "print(\"Dataset      x1       x2       x3       y\")\n",
        "print(D[:5])\n",
        "print(\"\\nInitial Weights\")\n",
        "print(W)\n",
        "print(\"\\nInitial Bias\")\n",
        "print(b[:5])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset      x1       x2       x3       y\n",
            "tensor([[ 1.5578, -1.9798, -0.2010,  2.7737],\n",
            "        [ 4.1796,  4.3103, -2.3964,  4.5342],\n",
            "        [-1.1956, -0.8957,  4.5102,  0.6863],\n",
            "        [-3.6190, -2.9313,  0.1390, -0.1500],\n",
            "        [ 0.1927, -1.5322, -0.5235,  1.3156]])\n",
            "\n",
            "Initial Weights\n",
            "tensor([0.1621, 0.8842, 0.7179], requires_grad=True)\n",
            "\n",
            "Initial Bias\n",
            "tensor([0.6858, 0.4205, 0.5289, 0.6379, 0.7051], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLP5dP9zNfzw",
        "colab_type": "code",
        "outputId": "e365564d-27ab-46e1-9553-4b2c78169670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# We'll perform step by step the procedure of linear regression\n",
        "def predict(X):\n",
        "    global W, b\n",
        "    return X@W.T + b\n",
        "\n",
        "def mse(y, y_hat):\n",
        "    N = y.numel()\n",
        "    return 1/(2*N) * torch.sum((y - y_hat).pow(2))\n",
        "\n",
        "x, y = X[0], Y[0]\n",
        "lr = 1e-03\n",
        "\n",
        "# Compute the value of y_hat (prediction)\n",
        "Y_hat = predict(X)\n",
        "print(f'y_hat = {Y_hat[:3]}')\n",
        "\n",
        "# Compute the mean squared error (sample loss)\n",
        "loss = mse(Y, Y_hat)\n",
        "print(f'loss = {loss}')\n",
        "\n",
        "# Compute the gradient of the loss w.r.o the parameters (W and b)\n",
        "loss.backward()\n",
        "print('Gradients:')\n",
        "print('W grad = ', W.grad[0].item())\n",
        "print('b grad', b.grad[0].item())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_hat = tensor([-0.9564,  3.1891,  2.7810], grad_fn=<SliceBackward>)\n",
            "loss = 10.507801055908203\n",
            "Gradients:\n",
            "W grad =  2.569791555404663\n",
            "b grad -0.03730182722210884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFPItXYOJSLj",
        "colab_type": "code",
        "outputId": "94d0044b-61c6-44ec-d5ab-dc620c92d4ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To make the linear regression model learn, run it inside a loop\n",
        "epochs = 100\n",
        "L = []\n",
        "for e in range(epochs):\n",
        "    Y_hat = predict(X)\n",
        "    loss = mse(Y, Y_hat)\n",
        "    L.append(loss.detach())\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        W -= lr*W.grad\n",
        "        b -= lr*b.grad\n",
        "        W.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "print(f'Training final loss = {loss}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training final loss = 5.392975330352783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9EW8nBQSsgv",
        "colab_type": "code",
        "outputId": "1f34dd75-ddae-4549-ee4a-488d79c49835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "# Plot how the loss change in each epoch\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        y = np.array(L), \n",
        "        name = \"Loss\",\n",
        "        line = go.scatter.Line(color=\"red\"),\n",
        "        showlegend = True\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title = \"Loss function over time\",\n",
        "    xaxis_title = \"Epochs (e)\",\n",
        "    yaxis_title = \"Loss value\",\n",
        "    font = dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"#7f7f7f\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"200687d1-d6ce-4b61-9318-4f88b5eb07d3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"200687d1-d6ce-4b61-9318-4f88b5eb07d3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '200687d1-d6ce-4b61-9318-4f88b5eb07d3',\n",
              "                        [{\"line\": {\"color\": \"red\"}, \"name\": \"Loss\", \"showlegend\": true, \"type\": \"scatter\", \"y\": [10.507801055908203, 10.293218612670898, 10.189221382141113, 10.087051391601562, 9.986676216125488, 9.888065338134766, 9.791182518005371, 9.696002006530762, 9.602492332458496, 9.510621070861816, 9.420361518859863, 9.331686019897461, 9.244563102722168, 9.158967018127441, 9.074870109558105, 8.99224853515625, 8.911070823669434, 8.831315040588379, 8.752955436706543, 8.675966262817383, 8.600322723388672, 8.52600383758545, 8.452982902526855, 8.381237983703613, 8.310746192932129, 8.241485595703125, 8.17343521118164, 8.106573104858398, 8.040875434875488, 7.976325511932373, 7.912901878356934, 7.850584030151367, 7.789351463317871, 7.72918701171875, 7.670069217681885, 7.611982345581055, 7.554906368255615, 7.498823642730713, 7.443717002868652, 7.38956880569458, 7.336362838745117, 7.284081935882568, 7.232707977294922, 7.182228088378906, 7.132625102996826, 7.083882808685303, 7.035987377166748, 6.988923072814941, 6.94267463684082, 6.897228717803955, 6.852570533752441, 6.808686256408691, 6.765562057495117, 6.723186492919922, 6.681542873382568, 6.64061975479126, 6.600407123565674, 6.56088924407959, 6.522054672241211, 6.483891010284424, 6.446387767791748, 6.409531593322754, 6.3733134269714355, 6.3377203941345215, 6.302742004394531, 6.26836633682251, 6.234582901000977, 6.201384544372559, 6.168756484985352, 6.136692523956299, 6.105179786682129, 6.074209690093994, 6.043772220611572, 6.013859272003174, 5.984461307525635, 5.955569267272949, 5.927173137664795, 5.899266242980957, 5.871838092803955, 5.844881534576416, 5.818386077880859, 5.7923479080200195, 5.766756057739258, 5.74160099029541, 5.7168779373168945, 5.6925811767578125, 5.668697834014893, 5.645225524902344, 5.622153282165527, 5.599478244781494, 5.577189922332764, 5.555281639099121, 5.533750057220459, 5.5125861167907715, 5.491783142089844, 5.471335411071777, 5.451237201690674, 5.43148136138916, 5.412062644958496, 5.392975330352783]}],\n",
              "                        {\"font\": {\"color\": \"#7f7f7f\", \"family\": \"Courier New, monospace\", \"size\": 18}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Loss function over time\"}, \"xaxis\": {\"title\": {\"text\": \"Epochs (e)\"}}, \"yaxis\": {\"title\": {\"text\": \"Loss value\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('200687d1-d6ce-4b61-9318-4f88b5eb07d3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH89gLCkXN-C",
        "colab_type": "text"
      },
      "source": [
        "## The easiest way 2 LR in Pytorch\n",
        "\n",
        "We can creare a model by using the torch.nn package. Is easiest than you think:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGiCQTBCXZ8O",
        "colab_type": "text"
      },
      "source": [
        "## Todo \n",
        "- [ ] Make a linear regression model using torch.nn\n",
        "- [ ] Download a linear separable dataset\n",
        "- [ ] Run a training loop \n",
        "- [ ] Plot the loss\n",
        "- [ ] Test the model accuracy with new data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iaGcGq1Xg_N",
        "colab_type": "code",
        "outputId": "9c8cad67-09f7-420d-e956-3aaaebe4d32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from torch.nn import Linear\n",
        "from torch.nn.functional import mse_loss\n",
        "\n",
        "model = Linear(3, 1)\n",
        "\n",
        "# Get the model parameters\n",
        "print(model.weight)\n",
        "print(f'\\n{model.bias}')\n",
        "\n",
        "# Also, you can run\n",
        "print(f'\\n{list(model.parameters())}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1664,  0.5583, -0.1737]], requires_grad=True)\n",
            "\n",
            "Parameter containing:\n",
            "tensor([0.1535], requires_grad=True)\n",
            "\n",
            "[Parameter containing:\n",
            "tensor([[-0.1664,  0.5583, -0.1737]], requires_grad=True), Parameter containing:\n",
            "tensor([0.1535], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "764IKSvpYaav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choosing a loss function and an optimizer\n",
        "\n",
        "# loss_fn = mse_loss\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-03)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}